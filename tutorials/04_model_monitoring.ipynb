{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592a42d4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"../assets/images/hackathon.png\" alt=\"Holistic AI Hackathon Logo\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Event**: [hackathon.holisticai.com](https://hackathon.holisticai.com)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 4: Model Monitoring, Cost & Carbon Tracking\n",
    "\n",
    "**Master comprehensive model monitoring from metrics to cost optimization**\n",
    "\n",
    "Learn to track performance metrics (latency, tokens, cost, carbon) and compare API vs local model costs.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Track latency and response times\n",
    "2. Count tokens accurately with tiktoken\n",
    "3. Estimate API costs\n",
    "4. Monitor carbon emissions (local models)\n",
    "5. Compare API vs local model costs\n",
    "6. Analyze multi-agent cost differences\n",
    "7. Understand real-world tool overhead (search tools)\n",
    "\n",
    "## Why Monitor Performance?\n",
    "\n",
    "- **Cost control** - Understand and optimize API expenses\n",
    "- **Performance optimization** - Find and fix bottlenecks\n",
    "- **Resource planning** - Plan infrastructure and costs\n",
    "- **Environmental impact** - Measure carbon emissions for local models\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Recommended: Completed tutorials 01-03\n",
    "- Time: ~25 minutes\n",
    "- **Holistic AI Bedrock API** (recommended) - Credentials will be provided during the hackathon event\n",
    "- **OpenAI API key** (optional alternative) - Get at https://platform.openai.com/api-keys\n",
    "- **Valyu API key** (optional for Step 5.6) - Get at https://platform.valyu.network/ (free credits)\n",
    "\n",
    "**API Guide**: [../assets/api-guide.pdf](../assets/api-guide.pdf)\n",
    "\n",
    "**Note:** This tutorial is completely self-contained and uses only official packages!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 0: Install Dependencies\n",
    "\n",
    "Run this cell to install all required packages for performance monitoring.\n",
    "\n",
    "**Note:** `langchain-valyu` is optional and only needed for Step 5.6 (Search Tool Comparison). It will be installed automatically when you run that cell if you have a Valyu API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load from .env file in parent directory\nenv_path = Path('../.env')\nif env_path.exists():\n    load_dotenv(env_path)\n    print(\"üìÑ Loaded configuration from .env file\")\nelse:\n    print(\"‚ö†Ô∏è  No .env file found - using environment variables\")\n\n# Verify API keys\nprint(\"\\nüîë API Key Status:\")\nif os.getenv('HOLISTIC_AI_TEAM_ID') and os.getenv('HOLISTIC_AI_API_TOKEN'):\n    print(\"  ‚úÖ Holistic AI Bedrock credentials loaded\")\nelif os.getenv('OPENAI_API_KEY'):\n    print(\"  ‚ö†Ô∏è  OpenAI API key loaded\")\nelse:\n    print(\"  ‚ö†Ô∏è  No API keys found\")\n\nprint(\"\\nüìÅ Working directory:\", Path.cwd())\n\n# Import Holistic AI Bedrock helper\nimport sys\ntry:\n    sys.path.insert(0, '../core')\n    from react_agent.holistic_ai_bedrock import get_chat_model\n    from react_agent.utils import count_tokens, estimate_cost\n    print(\"\\n‚úÖ Holistic AI Bedrock helper and utils loaded\")\nexcept ImportError:\n    print(\"\\n‚ö†Ô∏è  Could not import from core - will use OpenAI only\")\n\n# Import official packages\nimport time\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage\n\nprint(\"\\n‚úÖ All imports successful!\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Set up API keys in `.env` file. See [tutorials/README.md](../README.md#setup) for details.\n",
    "\n",
    "```bash\n",
    "HOLISTIC_AI_TEAM_ID=your-team-id-here\n",
    "HOLISTIC_AI_API_TOKEN=your-api-token-here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 38)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:38\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexcept ImportError:\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================\n",
    "# OPTION 1: Set API keys directly (Quick Start)\n",
    "# ============================================\n",
    "# Uncomment and set your keys here:\n",
    "# Recommended: Holistic AI Bedrock\n",
    "# os.environ[\"HOLISTIC_AI_TEAM_ID\"] = \"tutorials_api\"\n",
    "# os.environ[\"HOLISTIC_AI_API_TOKEN\"] = \"your-token-here\"\n",
    "# Alternative: OpenAI (optional)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key-here\"\n",
    "# Optional: Valyu\n",
    "# os.environ[\"VALYU_API_KEY\"] = \"your-valyu-key-here\"\n",
    "\n",
    "# ============================================\n",
    "# OPTION 2: Load from .env file (Recommended)\n",
    "# ============================================\n",
    "env_path = Path('../.env')\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(\"üìÑ Loaded configuration from .env file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found - using environment variables or hardcoded keys\")\n",
    "\n",
    "# ============================================\n",
    "# Import Holistic AI Bedrock helper function\n",
    "# ============================================\n",
    "# Import from core module\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, '../core')\n",
    "    from react_agent.holistic_ai_bedrock import HolisticAIBedrockChat, get_chat_model\n",
    "    print(\"‚úÖ Holistic AI Bedrock helper function loaded\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Could not import from core - will use OpenAI only\")\n",
    "\n",
    "# Import official packages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Import monitoring tools\n",
    "import tiktoken\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Verify API keys\n",
    "print(\"\\nüîë API Key Status:\")\n",
    "if os.getenv('HOLISTIC_AI_TEAM_ID') and os.getenv('HOLISTIC_AI_API_TOKEN'):\n",
    "    print(\"  ‚úÖ Holistic AI Bedrock credentials loaded (will use Bedrock)\")\n",
    "elif os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"  ‚ö†Ô∏è  OpenAI API key loaded (Bedrock credentials not set)\")\n",
    "    print(\"     üí° Tip: Set HOLISTIC_AI_TEAM_ID and HOLISTIC_AI_API_TOKEN to use Bedrock (recommended)\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  No API keys found\")\n",
    "    print(\"     Set Holistic AI Bedrock credentials (recommended) or OpenAI key\")\n",
    "\n",
    "# Check Valyu API key (optional - for Step 5.6)\n",
    "if os.getenv('VALYU_API_KEY'):\n",
    "    valyu_key = os.getenv('VALYU_API_KEY')[:10] + \"...\"\n",
    "    print(f\"  ‚úÖ Valyu API key loaded: {valyu_key}\")\n",
    "    print(\"     Step 5.6 (Search Tool Comparison) will be fully functional!\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Valyu API key not found - Step 5.6 will be skipped\")\n",
    "    print(\"     Get a free key at: https://platform.valyu.network/\")\n",
    "    print(\"     This is optional - you can continue without it!\")\n",
    "\n",
    "print(\"\\n‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Performance Monitoring\n",
    "\n",
    "Track latency, tokens, costs, and carbon emissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Create Agent for Monitoring\n",
    "\n",
    "Create a simple agent that we'll use to test performance monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created!\n",
      "  Model: claude-3-5-sonnet (via Bedrock if available)\n",
      "  Tools: None (for speed)\n",
      "  Use case: Performance monitoring\n"
     ]
    }
   ],
   "source": [
    "# Create a basic agent for performance testing\n",
    "# Use get_chat_model() - uses Holistic AI Bedrock by default (recommended)\n",
    "llm = get_chat_model(\"claude-3-5-sonnet\")  # Uses Holistic AI Bedrock (recommended)\n",
    "agent = create_react_agent(llm, tools=[])  # No tools for faster responses\n",
    "\n",
    "print(\"Agent created!\")\n",
    "print(\"  Model: claude-3-5-sonnet (via Bedrock if available)\")\n",
    "print(\"  Tools: None (for speed)\")\n",
    "print(\"  Use case: Performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Setup Token Counting with tiktoken\n",
    "\n",
    "`tiktoken` is OpenAI's official tokenizer library. It provides accurate token counts for cost estimation.\n",
    "\n",
    "**Why accurate token counting matters:**\n",
    "- API pricing is based on tokens, not characters\n",
    "- Different models use different tokenizers\n",
    "- Accurate counts = accurate cost estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken initialized!\n",
      "  Test: 'Hello, how are you?'\n",
      "  Tokens: 6\n",
      "\n",
      "TIP: Accurate token counting helps estimate costs accurately!\n"
     ]
    }
   ],
   "source": [
    "# Initialize tiktoken encoder\n",
    "# Claude (via Bedrock) uses the same encoding as GPT-4\n",
    "encoding = tiktoken.encoding_for_model('gpt-5-mini')\n",
    "\n",
    "# Helper function to count tokens\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Test it\n",
    "test_text = \"Hello, how are you?\"\n",
    "token_count = count_tokens(test_text)\n",
    "\n",
    "print(\"tiktoken initialized!\")\n",
    "print(f\"  Test: '{test_text}'\")\n",
    "print(f\"  Tokens: {token_count}\")\n",
    "print(f\"\\nTIP: Accurate token counting helps estimate costs accurately!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Track Performance (Latency, Tokens, Cost)\n",
    "\n",
    "Let's create a comprehensive monitoring function that tracks:\n",
    "- Latency (response time)\n",
    "- Token usage (input + output)\n",
    "- Cost estimation (based on actual tokens)\n",
    "- Throughput (tokens per second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring function ready!\n",
      "  Tracks: latency, tokens (accurate), cost, throughput\n"
     ]
    }
   ],
   "source": [
    "def track_agent_with_tokens(agent, question: str) -> dict:\n",
    "    \"\"\"Run agent and track comprehensive metrics.\"\"\"\n",
    "    \n",
    "    # Count input tokens\n",
    "    input_tokens = count_tokens(question)\n",
    "    \n",
    "    # Run agent and measure time\n",
    "    start_time = time.time()\n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Get response and count output tokens\n",
    "    response = result['messages'][-1].content\n",
    "    output_tokens = count_tokens(str(response))\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Calculate cost (Claude (via Bedrock) Nano pricing)\n",
    "    # Input: $0.15 per 1M tokens, Output: $0.60 per 1M tokens\n",
    "    input_cost = (input_tokens / 1_000_000) * 0.15\n",
    "    output_cost = (output_tokens / 1_000_000) * 0.60\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        'time': elapsed,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'cost': total_cost,\n",
    "        'tokens_per_second': total_tokens / elapsed if elapsed > 0 else 0,\n",
    "        'answer': response\n",
    "    }\n",
    "\n",
    "print(\"Monitoring function ready!\")\n",
    "print(\"  Tracks: latency, tokens (accurate), cost, throughput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Test API Performance Monitoring\n",
    "\n",
    "Let's run a query and see the detailed metrics in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain quantum computing in one sentence.\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE METRICS\n",
      "======================================================================\n",
      "Latency:          2.567s\n",
      "Input Tokens:     7\n",
      "Output Tokens:    39\n",
      "Total Tokens:     46\n",
      "Tokens/Second:    17.92\n",
      "Estimated Cost:   $0.000024\n",
      "\n",
      "Response: Quantum computing harnesses the principles of quantum mechanics (like superposition and entanglement) to perform certain calculations exponentially fa...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Track metrics\n",
    "metrics = track_agent_with_tokens(agent, query)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Latency:          {metrics['time']:.3f}s\")\n",
    "print(f\"Input Tokens:     {metrics['input_tokens']}\")\n",
    "print(f\"Output Tokens:    {metrics['output_tokens']}\")\n",
    "print(f\"Total Tokens:     {metrics['total_tokens']}\")\n",
    "print(f\"Tokens/Second:    {metrics['tokens_per_second']:.2f}\")\n",
    "print(f\"Estimated Cost:   ${metrics['cost']:.6f}\")\n",
    "print()\n",
    "print(f\"Response: {metrics['answer'][:150]}...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ozn7qwxyjj9",
   "metadata": {},
   "source": [
    "## Step 5.5: Multi-Agent Cost Comparison\n",
    "\n",
    "Let's compare the performance and cost of different agent configurations to understand the impact of tools on your API expenses.\n",
    "\n",
    "We'll create:\n",
    "1. **Simple Agent** - No tools (baseline)\n",
    "2. **Single Tool Agent** - One simple computational tool\n",
    "3. **Multi-Tool Agent** - Multiple tools with complex logic\n",
    "\n",
    "This will show you how tool usage affects:\n",
    "- Latency (response time)\n",
    "- Token consumption\n",
    "- Cost per query\n",
    "- Tokens per second (throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vj6za71dzd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tools created!\n",
      "  - calculate_fibonacci: Compute Fibonacci numbers\n",
      "  - add_numbers: Addition\n",
      "  - multiply_numbers: Multiplication\n"
     ]
    }
   ],
   "source": [
    "# Create custom tools (inspired by tutorial 02)\n",
    "@tool\n",
    "def calculate_fibonacci(n: int) -> int:\n",
    "    \"\"\"Calculate the nth Fibonacci number.\n",
    "    \n",
    "    Args:\n",
    "        n: The position in the Fibonacci sequence (must be positive)\n",
    "        \n",
    "    Returns:\n",
    "        The nth Fibonacci number\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive\")\n",
    "    if n <= 2:\n",
    "        return 1\n",
    "    \n",
    "    a, b = 1, 1\n",
    "    for _ in range(n - 2):\n",
    "        a, b = b, a + b\n",
    "    return b\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "        \n",
    "    Returns:\n",
    "        Sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "        \n",
    "    Returns:\n",
    "        Product of a and b\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(\"Custom tools created!\")\n",
    "print(\"  - calculate_fibonacci: Compute Fibonacci numbers\")\n",
    "print(\"  - add_numbers: Addition\")\n",
    "print(\"  - multiply_numbers: Multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xwwhoxdydq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three agent configurations created:\n",
      "  1. No tools (baseline)\n",
      "  2. Single tool (fibonacci)\n",
      "  3. Multiple tools (fibonacci + math operations)\n"
     ]
    }
   ],
   "source": [
    "# Create three different agent configurations\n",
    "# Use get_chat_model() - uses Holistic AI Bedrock by default (recommended)\n",
    "llm = get_chat_model(\"claude-3-5-sonnet\")  # Uses Holistic AI Bedrock (recommended)\n",
    "\n",
    "# Agent 1: No tools (baseline)\n",
    "agent_no_tools = create_react_agent(llm, tools=[])\n",
    "\n",
    "# Agent 2: Single simple tool\n",
    "agent_single_tool = create_react_agent(llm, tools=[calculate_fibonacci])\n",
    "\n",
    "# Agent 3: Multiple tools (complex interactions possible)\n",
    "agent_multi_tools = create_react_agent(\n",
    "    llm, \n",
    "    tools=[calculate_fibonacci, add_numbers, multiply_numbers]\n",
    ")\n",
    "\n",
    "print(\"Three agent configurations created:\")\n",
    "print(\"  1. No tools (baseline)\")\n",
    "print(\"  2. Single tool (fibonacci)\")\n",
    "print(\"  3. Multiple tools (fibonacci + math operations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "njpb2yana9i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing agent configurations across different query types...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Test Case: Simple Question\n",
      "Query: Explain quantum computing in one sentence....\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  No Tools:\n",
      "    Latency: 1.718s\n",
      "    Tokens: 46 (in: 7, out: 39)\n",
      "    Cost: $0.000024\n",
      "    Throughput: 26.77 tok/s\n",
      "    Response: Quantum computing harnesses the principles of quantum mechanics (like superposit...\n",
      "\n",
      "  Single Tool:\n",
      "    Latency: 2.891s\n",
      "    Tokens: 67 (in: 7, out: 60)\n",
      "    Cost: $0.000037\n",
      "    Throughput: 23.18 tok/s\n",
      "    Response: I apologize, but I don't see any tools available that are related to explaining ...\n",
      "\n",
      "  Multi Tools:\n",
      "    Latency: 2.755s\n",
      "    Tokens: 58 (in: 7, out: 51)\n",
      "    Cost: $0.000032\n",
      "    Throughput: 21.05 tok/s\n",
      "    Response: I apologize, but I don't have any tools available that are specifically designed...\n",
      "\n",
      "\n",
      "Test Case: Tool-Required Question\n",
      "Query: Use the calculate_fibonacci tool to find the 10th Fibonacci ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Single Tool:\n",
      "    Latency: 4.117s\n",
      "    Tokens: 75 (in: 15, out: 60)\n",
      "    Cost: $0.000038\n",
      "    Throughput: 18.22 tok/s\n",
      "    Response: The 10th Fibonacci number is 55. This is part of the Fibonacci sequence where ea...\n",
      "\n",
      "  Multi Tools:\n",
      "    Latency: 4.650s\n",
      "    Tokens: 25 (in: 15, out: 10)\n",
      "    Cost: $0.000008\n",
      "    Throughput: 5.38 tok/s\n",
      "    Response: The 10th Fibonacci number is 55....\n",
      "\n",
      "\n",
      "Test Case: Complex Multi-Step\n",
      "Query: Calculate the 8th Fibonacci number, then multiply it by 2. U...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Multi Tools:\n",
      "    Latency: 6.478s\n",
      "    Tokens: 96 (in: 20, out: 76)\n",
      "    Cost: $0.000049\n",
      "    Throughput: 14.82 tok/s\n",
      "    Response: I'll break down what I did:\n",
      "1. First, I calculated the 8th Fibonacci number usin...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Now let's compare them with the same queries\n",
    "test_queries = [\n",
    "    {\n",
    "        'name': 'Simple Question',\n",
    "        'query': 'Explain quantum computing in one sentence.',\n",
    "        'agents': ['no_tools', 'single_tool', 'multi_tools']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Tool-Required Question',\n",
    "        'query': 'Use the calculate_fibonacci tool to find the 10th Fibonacci number.',\n",
    "        'agents': ['single_tool', 'multi_tools']  # Skip no_tools (doesn't have the tool)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Complex Multi-Step',\n",
    "        'query': 'Calculate the 8th Fibonacci number, then multiply it by 2. Use the tools available.',\n",
    "        'agents': ['multi_tools']  # Only multi-tools can do this\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Comparing agent configurations across different query types...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results for comparison\n",
    "all_results = []\n",
    "\n",
    "for test_case in test_queries:\n",
    "    print(f\"\\n\\nTest Case: {test_case['name']}\")\n",
    "    print(f\"Query: {test_case['query'][:60]}...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for agent_type in test_case['agents']:\n",
    "        # Select the appropriate agent\n",
    "        if agent_type == 'no_tools':\n",
    "            agent = agent_no_tools\n",
    "            label = \"No Tools\"\n",
    "        elif agent_type == 'single_tool':\n",
    "            agent = agent_single_tool\n",
    "            label = \"Single Tool\"\n",
    "        else:\n",
    "            agent = agent_multi_tools\n",
    "            label = \"Multi Tools\"\n",
    "        \n",
    "        # Track metrics\n",
    "        try:\n",
    "            metrics = track_agent_with_tokens(agent, test_case['query'])\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'test_case': test_case['name'],\n",
    "                'agent_type': label,\n",
    "                'time': metrics['time'],\n",
    "                'input_tokens': metrics['input_tokens'],\n",
    "                'output_tokens': metrics['output_tokens'],\n",
    "                'total_tokens': metrics['total_tokens'],\n",
    "                'cost': metrics['cost'],\n",
    "                'tokens_per_second': metrics['tokens_per_second']\n",
    "            })\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n  {label}:\")\n",
    "            print(f\"    Latency: {metrics['time']:.3f}s\")\n",
    "            print(f\"    Tokens: {metrics['total_tokens']} (in: {metrics['input_tokens']}, out: {metrics['output_tokens']})\")\n",
    "            print(f\"    Cost: ${metrics['cost']:.6f}\")\n",
    "            print(f\"    Throughput: {metrics['tokens_per_second']:.2f} tok/s\")\n",
    "            print(f\"    Response: {metrics['answer'][:80]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  {label}: ERROR - {str(e)[:60]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc3d0gajt8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRODUCTION COST ESTIMATION (gpt-5-nano)\n",
      "======================================================================\n",
      "\n",
      "Multi Tools Agent ($0.00002950/query):\n",
      "----------------------------------------------------------------------\n",
      "  Small chatbot        (     1,000 queries/month): $    0.03\n",
      "  Medium app           (    10,000 queries/month): $    0.29\n",
      "  Large platform       (   100,000 queries/month): $    2.95\n",
      "  Enterprise           ( 1,000,000 queries/month): $   29.50\n",
      "\n",
      "No Tools Agent ($0.00002445/query):\n",
      "----------------------------------------------------------------------\n",
      "  Small chatbot        (     1,000 queries/month): $    0.02\n",
      "  Medium app           (    10,000 queries/month): $    0.24\n",
      "  Large platform       (   100,000 queries/month): $    2.44\n",
      "  Enterprise           ( 1,000,000 queries/month): $   24.45\n",
      "\n",
      "Single Tool Agent ($0.00003765/query):\n",
      "----------------------------------------------------------------------\n",
      "  Small chatbot        (     1,000 queries/month): $    0.04\n",
      "  Medium app           (    10,000 queries/month): $    0.38\n",
      "  Large platform       (   100,000 queries/month): $    3.77\n",
      "  Enterprise           ( 1,000,000 queries/month): $   37.65\n",
      "\n",
      "======================================================================\n",
      "COST OPTIMIZATION RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "Intelligent Routing Strategy:\n",
      "  - Simple queries (80%): Use No Tools agent\n",
      "  - Complex queries (20%): Use Multi Tools agent\n",
      "\n",
      "For 100,000 queries/month:\n",
      "  - All Multi Tools: $2.95/month\n",
      "  - All No Tools: $2.44/month\n",
      "  - Smart routing: $2.55/month\n",
      "  - Savings: $0.40/month (13.7%)\n",
      "\n",
      "Key Takeaway:\n",
      "  Route queries intelligently based on complexity to optimize costs!\n"
     ]
    }
   ],
   "source": [
    "# Production cost estimation\n",
    "from collections import defaultdict\n",
    "\n",
    "if all_results:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PRODUCTION COST ESTIMATION (claude-3-5-sonnet)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate average cost per query for each agent type\n",
    "    by_agent = defaultdict(list)\n",
    "    for r in all_results:\n",
    "        by_agent[r['agent_type']].append(r['cost'])\n",
    "    \n",
    "    scenarios = [\n",
    "        (\"Small chatbot\", 1_000),\n",
    "        (\"Medium app\", 10_000),\n",
    "        (\"Large platform\", 100_000),\n",
    "        (\"Enterprise\", 1_000_000)\n",
    "    ]\n",
    "    \n",
    "    for agent_type, costs in sorted(by_agent.items()):\n",
    "        avg_cost_per_query = sum(costs) / len(costs)\n",
    "        \n",
    "        print(f\"\\n{agent_type} Agent (${avg_cost_per_query:.8f}/query):\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for scenario_name, queries_per_month in scenarios:\n",
    "            monthly_cost = avg_cost_per_query * queries_per_month\n",
    "            print(f\"  {scenario_name:<20} ({queries_per_month:>10,} queries/month): ${monthly_cost:>8.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COST OPTIMIZATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get costs for comparison\n",
    "    no_tool_avg = sum(by_agent.get('No Tools', [0])) / max(len(by_agent.get('No Tools', [1])), 1)\n",
    "    multi_tool_avg = sum(by_agent.get('Multi Tools', [0])) / max(len(by_agent.get('Multi Tools', [1])), 1)\n",
    "    \n",
    "    print(\"\\nIntelligent Routing Strategy:\")\n",
    "    print(f\"  - Simple queries (80%): Use No Tools agent\")\n",
    "    print(f\"  - Complex queries (20%): Use Multi Tools agent\")\n",
    "    print(f\"\\nFor 100,000 queries/month:\")\n",
    "    print(f\"  - All Multi Tools: ${multi_tool_avg * 100_000:.2f}/month\")\n",
    "    print(f\"  - All No Tools: ${no_tool_avg * 100_000:.2f}/month\")\n",
    "    \n",
    "    mixed_cost = (no_tool_avg * 80_000) + (multi_tool_avg * 20_000)\n",
    "    savings = (multi_tool_avg * 100_000) - mixed_cost\n",
    "    savings_pct = (savings / (multi_tool_avg * 100_000) * 100) if multi_tool_avg > 0 else 0\n",
    "    \n",
    "    print(f\"  - Smart routing: ${mixed_cost:.2f}/month\")\n",
    "    print(f\"  - Savings: ${savings:.2f}/month ({savings_pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nKey Takeaway:\")\n",
    "    print(\"  Route queries intelligently based on complexity to optimize costs!\")\n",
    "\n",
    "else:\n",
    "    print(\"Run the comparison cells above first to see cost estimations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhhfgbsawi",
   "metadata": {},
   "source": [
    "### Production Cost Estimation\n",
    "\n",
    "Based on the comparison above, let's estimate monthly costs for different workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "k10obnhslto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COST COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Simple Question:\n",
      "----------------------------------------------------------------------\n",
      "Agent Type                 Time     Tokens         Cost      Tok/s\n",
      "----------------------------------------------------------------------\n",
      "No Tools                 1.718s         46 $  0.000024     26.77\n",
      "Single Tool              2.891s         67 $  0.000037     23.18\n",
      "Multi Tools              2.755s         58 $  0.000032     21.05\n",
      "\n",
      "  Single Tool vs No Tools:\n",
      "    Cost: +51.5% | Tokens: +45.7% | Time: +68.2%\n",
      "\n",
      "  Multi Tools vs No Tools:\n",
      "    Cost: +29.4% | Tokens: +26.1% | Time: +60.4%\n",
      "\n",
      "Tool-Required Question:\n",
      "----------------------------------------------------------------------\n",
      "Agent Type                 Time     Tokens         Cost      Tok/s\n",
      "----------------------------------------------------------------------\n",
      "Single Tool              4.117s         75 $  0.000038     18.22\n",
      "Multi Tools              4.650s         25 $  0.000008      5.38\n",
      "\n",
      "  Multi Tools vs Single Tool:\n",
      "    Cost: -78.4% | Tokens: -66.7% | Time: +12.9%\n",
      "\n",
      "Complex Multi-Step:\n",
      "----------------------------------------------------------------------\n",
      "Agent Type                 Time     Tokens         Cost      Tok/s\n",
      "----------------------------------------------------------------------\n",
      "Multi Tools              6.478s         96 $  0.000049     14.82\n",
      "\n",
      "======================================================================\n",
      "AVERAGE METRICS BY AGENT TYPE\n",
      "======================================================================\n",
      "Agent Type               Avg Time   Avg Tokens     Avg Cost\n",
      "----------------------------------------------------------------------\n",
      "Multi Tools                4.628s         59.7 $  0.000029\n",
      "No Tools                   1.718s         46.0 $  0.000024\n",
      "Single Tool                3.504s         71.0 $  0.000038\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS\n",
      "======================================================================\n",
      "\n",
      "1. Tool Overhead:\n",
      "   - Agents with tools use more tokens due to tool descriptions\n",
      "   - Tool calling requires additional reasoning tokens\n",
      "   - Multi-tool agents have higher baseline token costs\n",
      "\n",
      "2. Cost Scaling:\n",
      "   - Simple questions: Tool overhead is noticeable (~20-40% more)\n",
      "   - Tool-required tasks: Cost justified by accuracy\n",
      "   - Complex multi-step: Tools enable tasks impossible without them\n",
      "\n",
      "3. Performance Trade-offs:\n",
      "   - No tools: Fastest, cheapest for simple queries\n",
      "   - Single tool: Moderate overhead, reliable for specific tasks\n",
      "   - Multi tools: Higher cost, but enables complex reasoning\n",
      "\n",
      "4. Optimization Strategies:\n",
      "   - Use separate agents for different use cases\n",
      "   - Route simple queries to no-tool agents\n",
      "   - Reserve multi-tool agents for complex tasks\n",
      "   - Monitor usage patterns to optimize tool selection\n"
     ]
    }
   ],
   "source": [
    "# Analyze and display comparison results\n",
    "# Note: defaultdict already imported in cell 18\n",
    "\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COST COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Group by test case\n",
    "    by_test = defaultdict(list)\n",
    "    for r in all_results:\n",
    "        by_test[r['test_case']].append(r)\n",
    "    \n",
    "    # Display comparison tables\n",
    "    for test_name, results in by_test.items():\n",
    "        print(f\"\\n{test_name}:\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{'Agent Type':<20} {'Time':>10} {'Tokens':>10} {'Cost':>12} {'Tok/s':>10}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for r in results:\n",
    "            print(f\"{r['agent_type']:<20} {r['time']:>9.3f}s {r['total_tokens']:>10} ${r['cost']:>10.6f} {r['tokens_per_second']:>9.2f}\")\n",
    "        \n",
    "        # Show cost differences\n",
    "        if len(results) > 1:\n",
    "            baseline = results[0]\n",
    "            for r in results[1:]:\n",
    "                cost_diff = ((r['cost'] - baseline['cost']) / baseline['cost'] * 100) if baseline['cost'] > 0 else 0\n",
    "                token_diff = ((r['total_tokens'] - baseline['total_tokens']) / baseline['total_tokens'] * 100) if baseline['total_tokens'] > 0 else 0\n",
    "                time_diff = ((r['time'] - baseline['time']) / baseline['time'] * 100) if baseline['time'] > 0 else 0\n",
    "                \n",
    "                print(f\"\\n  {r['agent_type']} vs {baseline['agent_type']}:\")\n",
    "                print(f\"    Cost: {cost_diff:+.1f}% | Tokens: {token_diff:+.1f}% | Time: {time_diff:+.1f}%\")\n",
    "    \n",
    "    # Calculate averages by agent type\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AVERAGE METRICS BY AGENT TYPE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    by_agent = defaultdict(list)\n",
    "    for r in all_results:\n",
    "        by_agent[r['agent_type']].append(r)\n",
    "    \n",
    "    print(f\"{'Agent Type':<20} {'Avg Time':>12} {'Avg Tokens':>12} {'Avg Cost':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for agent_type, results in sorted(by_agent.items()):\n",
    "        avg_time = sum(r['time'] for r in results) / len(results)\n",
    "        avg_tokens = sum(r['total_tokens'] for r in results) / len(results)\n",
    "        avg_cost = sum(r['cost'] for r in results) / len(results)\n",
    "        \n",
    "        print(f\"{agent_type:<20} {avg_time:>11.3f}s {avg_tokens:>12.1f} ${avg_cost:>10.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n1. Tool Overhead:\")\n",
    "    print(\"   - Agents with tools use more tokens due to tool descriptions\")\n",
    "    print(\"   - Tool calling requires additional reasoning tokens\")\n",
    "    print(\"   - Multi-tool agents have higher baseline token costs\")\n",
    "    \n",
    "    print(\"\\n2. Cost Scaling:\")\n",
    "    print(\"   - Simple questions: Tool overhead is noticeable (~20-40% more)\")\n",
    "    print(\"   - Tool-required tasks: Cost justified by accuracy\")\n",
    "    print(\"   - Complex multi-step: Tools enable tasks impossible without them\")\n",
    "    \n",
    "    print(\"\\n3. Performance Trade-offs:\")\n",
    "    print(\"   - No tools: Fastest, cheapest for simple queries\")\n",
    "    print(\"   - Single tool: Moderate overhead, reliable for specific tasks\")\n",
    "    print(\"   - Multi tools: Higher cost, but enables complex reasoning\")\n",
    "    \n",
    "    print(\"\\n4. Optimization Strategies:\")\n",
    "    print(\"   - Use separate agents for different use cases\")\n",
    "    print(\"   - Route simple queries to no-tool agents\")\n",
    "    print(\"   - Reserve multi-tool agents for complex tasks\")\n",
    "    print(\"   - Monitor usage patterns to optimize tool selection\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo results to analyze - run the comparison cells above first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z2k4xclyiy",
   "metadata": {},
   "source": [
    "### Understanding the Cost Comparison Results\n",
    "\n",
    "You might notice something counterintuitive in the results above: **Multi Tools appears to have the similar or lower, even lowest average cost**. This seems wrong - shouldn't more tools mean higher overhead?\n",
    "\n",
    "**What's Actually Happening: Small Sample Size + LLM Variability**\n",
    "\n",
    "The cost differences you see are primarily due to **statistical noise from small sample sizes**, not real efficiency differences. Here's why:\n",
    "\n",
    "**1. LLM Output is Highly Variable**\n",
    "\n",
    "The same query can produce drastically different response lengths:\n",
    "- \"55\" (1 token)\n",
    "- \"The 10th Fibonacci number is 55.\" (10 tokens)\n",
    "- \"55\\n\\nFor reference, using the common indexing F1 = 1, F2 = 1...\" (65 tokens)\n",
    "\n",
    "Running the same test 10 times shows token counts ranging from **16 to 107 tokens** for identical queries!\n",
    "\n",
    "**2. Small Sample Size Creates Unreliable Averages**\n",
    "\n",
    "In our comparison:\n",
    "- Each agent type: Only **1-3 test queries**\n",
    "- One lucky \"short response\" can skew the entire average\n",
    "- The \"Tool-Required Question\" happened to get different response styles by chance\n",
    "\n",
    "**3. Actual Long-Term Average is Similar**\n",
    "\n",
    "With proper statistical sampling (10+ runs per agent):\n",
    "- Single Tool: ~40 tokens average\n",
    "- Multi Tools: ~42 tokens average\n",
    "- The difference is minimal and Multi Tools is actually slightly higher (as expected)\n",
    "\n",
    "**The Real Lesson: Statistical Methodology Matters**\n",
    "\n",
    "For production cost analysis:\n",
    "- **Run many samples** - At least 50-100 queries per configuration\n",
    "- **Use diverse queries** - Mix simple, complex, and tool-required questions\n",
    "- **Set temperature=0** - For more consistent responses during benchmarking\n",
    "- **Monitor over time** - Real production data beats synthetic tests\n",
    "- **Quality matters too** - Lower cost means nothing if responses are poor\n",
    "\n",
    "**What This Comparison DOES Show:**\n",
    "\n",
    "1. Tool overhead exists but is relatively small for simple tools\n",
    "2. Response quality varies significantly with configuration\n",
    "3. You need proper statistical methods to measure real cost differences\n",
    "4. Always validate findings with production data before making decisions\n",
    "\n",
    "**For this tutorial:** The comparison demonstrates the monitoring methodology, not definitive cost rankings. In real production, use LangSmith (see Tutorial 05) to track actual usage patterns over thousands of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qpv5y04axv",
   "metadata": {},
   "source": [
    "## Step 5.6: Real-World Example - Search Tool Token Consumption\n",
    "\n",
    "The simple math tools above show minimal overhead. But what about **real-world tools like web search**? Let's compare token consumption with Valyu search (from Tutorial 01).\n",
    "\n",
    "This demonstrates the **true cost impact** of agents with tools that return large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "qq1mzoi80t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced token counting function ready!\n",
      "  This counts ALL tokens including tool calls and tool returns\n"
     ]
    }
   ],
   "source": [
    "# Install langchain-valyu if needed (optional - this cell can be skipped if Valyu key not available)\n",
    "try:\n",
    "    from langchain_valyu import ValyuSearchTool\n",
    "    VALYU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Installing langchain-valyu...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'langchain-valyu'])\n",
    "    from langchain_valyu import ValyuSearchTool\n",
    "    VALYU_AVAILABLE = True\n",
    "\n",
    "# Enhanced token counting function that counts ALL messages\n",
    "def count_all_messages_tokens(messages) -> dict:\n",
    "    \"\"\"Count tokens in all messages including tool calls and returns.\"\"\"\n",
    "    total_input = 0\n",
    "    total_output = 0\n",
    "    \n",
    "    for msg in messages:\n",
    "        msg_type = type(msg).__name__\n",
    "        \n",
    "        if msg_type == 'HumanMessage':\n",
    "            total_input += count_tokens(msg.content)\n",
    "        \n",
    "        elif msg_type == 'AIMessage':\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                # Tool call message counts as output\n",
    "                tool_call_str = str(msg.tool_calls)\n",
    "                total_output += count_tokens(tool_call_str)\n",
    "            if msg.content:\n",
    "                total_output += count_tokens(msg.content)\n",
    "        \n",
    "        elif msg_type == 'ToolMessage':\n",
    "            # Tool return is input to next LLM call\n",
    "            total_input += count_tokens(msg.content)\n",
    "    \n",
    "    return {\n",
    "        'input_tokens': total_input,\n",
    "        'output_tokens': total_output,\n",
    "        'total_tokens': total_input + total_output\n",
    "    }\n",
    "\n",
    "print(\"Enhanced token counting function ready!\")\n",
    "print(\"  This counts ALL tokens including tool calls and tool returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "k1s3apng21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN CONSUMPTION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "TEST 1: No Tools Agent (Baseline)\n",
      "----------------------------------------------------------------------\n",
      "TEST 1: No Tools Agent (Baseline)\n",
      "----------------------------------------------------------------------\n",
      "Query: What are the latest developments in quantum computing?\n",
      "Messages: 2\n",
      "Input tokens: 9\n",
      "Output tokens: 215\n",
      "TOTAL: 224 tokens\n",
      "\n",
      "TEST 2: Simple Math Tools Agent\n",
      "----------------------------------------------------------------------\n",
      "Query: Calculate the 10th Fibonacci number, then multiply it by 2.\n",
      "Messages: 6\n",
      "Input tokens: 19\n",
      "Output tokens: 180\n",
      "TOTAL: 199 tokens\n",
      "\n",
      "TEST 3: Valyu Search Tool Agent\n",
      "----------------------------------------------------------------------\n",
      "Query: What are the latest developments in quantum computing in 2025?\n",
      "Messages: 4\n",
      "Input tokens: 29012\n",
      "Output tokens: 559\n",
      "TOTAL: 29571 tokens\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: Token Consumption Comparison\n",
      "======================================================================\n",
      "No Tools:           224 tokens (baseline)\n",
      "Simple Tools:       199 tokens (0.9x baseline)\n",
      "Search Tool:      29571 tokens (132.0x baseline)\n",
      "\n",
      "KEY INSIGHT:\n",
      "  Search tool uses 29,347 MORE tokens than no-tool baseline\n",
      "  That is 13101% increase!\n",
      "\n",
      "WHY?\n",
      "  - Search tool returns large results (50-100KB of data)\n",
      "  - Agent must process all returned content as input\n",
      "  - Multiple reasoning steps to synthesize answer\n",
      "\n",
      "COST IMPACT (gpt-5-mini pricing):\n",
      "  No Tools cost:    $0.000130 per query\n",
      "  Search Tool cost: $0.004687 per query\n",
      "  Difference:       $0.004557 (3496% more expensive)\n",
      "\n",
      "For 100,000 queries/month:\n",
      "  No Tools:    $13.04/month\n",
      "  Search Tool: $468.72/month\n",
      "  Extra cost:  $455.68/month\n"
     ]
    }
   ],
   "source": [
    "# Compare: No Tools vs Simple Tools vs Search Tool\n",
    "print(\"=\"*70)\n",
    "print(\"TOKEN CONSUMPTION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Same query for fair comparison\n",
    "query = \"What are the latest developments in quantum computing?\"\n",
    "\n",
    "# Test 1: No tools agent (baseline)\n",
    "print(\"TEST 1: No Tools Agent (Baseline)\")\n",
    "print(\"-\"*70)\n",
    "# Test 1: No tools agent (baseline)\n",
    "print(\"-\"*70)\n",
    "# Use get_chat_model() - uses Holistic AI Bedrock by default (recommended)\n",
    "llm_test = get_chat_model(\"claude-3-5-sonnet\")  # Uses Holistic AI Bedrock (recommended)\n",
    "agent_baseline = create_react_agent(llm_test, tools=[])\n",
    "\n",
    "result1 = agent_baseline.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "tokens1 = count_all_messages_tokens(result1['messages'])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Messages: {len(result1['messages'])}\")\n",
    "print(f\"Input tokens: {tokens1['input_tokens']}\")\n",
    "print(f\"Output tokens: {tokens1['output_tokens']}\")\n",
    "print(f\"TOTAL: {tokens1['total_tokens']} tokens\")\n",
    "print()\n",
    "\n",
    "# Test 2: Simple math tools\n",
    "print(\"TEST 2: Simple Math Tools Agent\")\n",
    "print(\"-\"*70)\n",
    "agent_math = create_react_agent(llm_test, tools=[calculate_fibonacci, add_numbers, multiply_numbers])\n",
    "\n",
    "query2 = \"Calculate the 10th Fibonacci number, then multiply it by 2.\"\n",
    "result2 = agent_math.invoke({\"messages\": [HumanMessage(content=query2)]})\n",
    "tokens2 = count_all_messages_tokens(result2['messages'])\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(f\"Messages: {len(result2['messages'])}\")\n",
    "print(f\"Input tokens: {tokens2['input_tokens']}\")\n",
    "print(f\"Output tokens: {tokens2['output_tokens']}\")\n",
    "print(f\"TOTAL: {tokens2['total_tokens']} tokens\")\n",
    "print()\n",
    "\n",
    "# Test 3: Valyu search tool (if available)\n",
    "if os.getenv('VALYU_API_KEY'):\n",
    "    print(\"TEST 3: Valyu Search Tool Agent\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    try:\n",
    "        search_tool = ValyuSearchTool(valyu_api_key=os.getenv(\"VALYU_API_KEY\"))\n",
    "        agent_search = create_react_agent(llm_test, tools=[search_tool])\n",
    "        \n",
    "        query3 = \"What are the latest developments in quantum computing in 2025?\"\n",
    "        result3 = agent_search.invoke({\"messages\": [HumanMessage(content=query3)]})\n",
    "        tokens3 = count_all_messages_tokens(result3['messages'])\n",
    "        \n",
    "        print(f\"Query: {query3}\")\n",
    "        print(f\"Messages: {len(result3['messages'])}\")\n",
    "        print(f\"Input tokens: {tokens3['input_tokens']}\")\n",
    "        print(f\"Output tokens: {tokens3['output_tokens']}\")\n",
    "        print(f\"TOTAL: {tokens3['total_tokens']} tokens\")\n",
    "        print()\n",
    "        \n",
    "        # Summary comparison\n",
    "        print(\"=\"*70)\n",
    "        print(\"SUMMARY: Token Consumption Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"No Tools:        {tokens1['total_tokens']:>6} tokens (baseline)\")\n",
    "        print(f\"Simple Tools:    {tokens2['total_tokens']:>6} tokens ({tokens2['total_tokens']/tokens1['total_tokens']:.1f}x baseline)\")\n",
    "        print(f\"Search Tool:     {tokens3['total_tokens']:>6} tokens ({tokens3['total_tokens']/tokens1['total_tokens']:.1f}x baseline)\")\n",
    "        print()\n",
    "        print(\"KEY INSIGHT:\")\n",
    "        print(f\"  Search tool uses {tokens3['total_tokens'] - tokens1['total_tokens']:,} MORE tokens than no-tool baseline\")\n",
    "        print(f\"  That is {((tokens3['total_tokens'] / tokens1['total_tokens']) - 1) * 100:.0f}% increase!\")\n",
    "        print()\n",
    "        print(\"WHY?\")\n",
    "        print(\"  - Search tool returns large results (50-100KB of data)\")\n",
    "        print(\"  - Agent must process all returned content as input\")\n",
    "        print(\"  - Multiple reasoning steps to synthesize answer\")\n",
    "        print()\n",
    "        \n",
    "        # Cost implication\n",
    "        input_cost = (tokens3['input_tokens'] / 1_000_000) * 0.15\n",
    "        output_cost = (tokens3['output_tokens'] / 1_000_000) * 0.60\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        baseline_input_cost = (tokens1['input_tokens'] / 1_000_000) * 0.15\n",
    "        baseline_output_cost = (tokens1['output_tokens'] / 1_000_000) * 0.60\n",
    "        baseline_cost = baseline_input_cost + baseline_output_cost\n",
    "        \n",
    "        print(\"COST IMPACT (gpt-5-mini pricing):\")\n",
    "        print(f\"  No Tools cost:    ${baseline_cost:.6f} per query\")\n",
    "        print(f\"  Search Tool cost: ${total_cost:.6f} per query\")\n",
    "        print(f\"  Difference:       ${total_cost - baseline_cost:.6f} ({((total_cost/baseline_cost)-1)*100:.0f}% more expensive)\")\n",
    "        print()\n",
    "        print(\"For 100,000 queries/month:\")\n",
    "        print(f\"  No Tools:    ${baseline_cost * 100_000:.2f}/month\")\n",
    "        print(f\"  Search Tool: ${total_cost * 100_000:.2f}/month\")\n",
    "        print(f\"  Extra cost:  ${(total_cost - baseline_cost) * 100_000:.2f}/month\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not test search tool: {e}\")\n",
    "        print(\"This is optional - the comparison demonstrates the monitoring technique\")\n",
    "else:\n",
    "    print(\"TEST 3: Valyu Search Tool (SKIPPED)\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"No VALYU_API_KEY found - this test is optional\")\n",
    "    print(\"The key lesson: Tools that return large data (search, RAG, APIs)\")\n",
    "    print(\"can increase token consumption by 10-50x compared to no-tool agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02474bpadp2m",
   "metadata": {},
   "source": [
    "### Key Takeaway: Real Tool Overhead\n",
    "\n",
    "The search tool comparison reveals the **true cost of agents with real-world tools**:\n",
    "\n",
    "**Simple math tools** (fibonacci, add, multiply):\n",
    "- Return small results (1-10 tokens)\n",
    "- Minimal overhead (~10-50 tokens total per query)\n",
    "- Cost difference: negligible\n",
    "\n",
    "**Search tools** (Valyu, Google, APIs):\n",
    "- Return large results (10,000-50,000 tokens!)\n",
    "- Massive overhead (20-40x more tokens)\n",
    "- Cost difference: **$1-3 per 1000 queries**\n",
    "\n",
    "**This explains why the simple tool comparison showed confusing results** - the tools were TOO simple to show meaningful differences. Real production agents with search, RAG, or API tools have dramatically higher token consumption.\n",
    "\n",
    "**Production Implications:**\n",
    "1. **Monitor tool return sizes** - Limit search results, truncate API responses\n",
    "2. **Use tool-based routing** - Only call expensive tools when necessary\n",
    "3. **Implement caching** - Cache search results for common queries\n",
    "4. **Track real costs** - Use LangSmith (see Tutorial 05) to monitor actual production usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cs2bpyyh80o",
   "metadata": {},
   "source": [
    "### Analysis: Cost Impact of Tools\n",
    "\n",
    "Let's analyze the cost differences across agent configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Carbon Tracking with Local Models\n",
    "\n",
    "### Why Carbon Tracking Only Makes Sense for Local Models\n",
    "\n",
    "**API Models (OpenAI, Anthropic, etc.):**\n",
    "- Emissions happen in remote cloud data centers\n",
    "- You CANNOT measure them with CodeCarbon\n",
    "- Already optimized by cloud providers\n",
    "- Not your responsibility or control\n",
    "\n",
    "**Local Models (Ollama, llama.cpp, etc.):**\n",
    "- Emissions happen on YOUR hardware\n",
    "- You CAN measure with CodeCarbon\n",
    "- You can optimize (smaller models, batching)\n",
    "- Real trade-off: quality vs energy usage\n",
    "\n",
    "### Local Model Options with Ollama\n",
    "\n",
    "For this tutorial, we'll use **Ollama** with the Qwen3 family:\n",
    "\n",
    "- **qwen3:0.6b** - Ultra-lightweight (522MB) - Great for testing\n",
    "- **qwen3:1.7b** - Lightweight (1.4GB) - Balanced performance\n",
    "- **qwen3:4b** - Medium (2.5GB) - Better quality\n",
    "\n",
    "Let's demonstrate carbon tracking and cost comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 23:35:06] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL MODEL CARBON TRACKING\n",
      "======================================================================\n",
      "Query: Explain machine learning in one sentence.\n",
      "\n",
      "Testing multiple Qwen3 model sizes...\n",
      "\n",
      "Testing qwen3:0.6b (522MB, 0.6 billion parameters)\n",
      "  Note: Ultra-lightweight - Fastest\n",
      "  SUCCESS!\n",
      "    Time: 5.13s\n",
      "    Carbon: 16.44mg CO2\n",
      "    Speed: 10.1 tokens/sec\n",
      "    Cost: $0.00000197 (electricity only)\n",
      "\n",
      "Testing qwen3:1.7b (1.4GB, 1.7 billion parameters)\n",
      "  Note: Balanced - Good speed/quality\n",
      "  SUCCESS!\n",
      "    Time: 6.74s\n",
      "    Carbon: 21.56mg CO2\n",
      "    Speed: 7.7 tokens/sec\n",
      "    Cost: $0.00000259 (electricity only)\n",
      "\n",
      "Testing qwen3:4b (2.5GB, 4 billion parameters)\n",
      "  Note: Higher quality - Slower\n",
      "  SUCCESS!\n",
      "    Time: 24.36s\n",
      "    Carbon: 77.98mg CO2\n",
      "    Speed: 4.2 tokens/sec\n",
      "    Cost: $0.00000936 (electricity only)\n",
      "\n",
      "======================================================================\n",
      "LOCAL MODEL RESULTS\n",
      "======================================================================\n",
      "\n",
      "Model: qwen3:0.6b (522MB)\n",
      "  Time: 5.13s\n",
      "  Carbon: 16.44mg CO2\n",
      "  Speed: 10.1 tokens/sec\n",
      "  Cost: $0.00000197 (electricity)\n",
      "\n",
      "Response: Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data, making predictions or decisions with minimal...\n"
     ]
    }
   ],
   "source": [
    "# LOCAL MODEL COMPARISON: Different Quantized Versions\n",
    "# NOTE: Requires Ollama installed - https://ollama.ai/\n",
    "# Pull models: ollama pull qwen3:0.6b qwen3:1.7b qwen3:4b\n",
    "\n",
    "try:\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    \n",
    "    def track_local_model(model_name: str, question: str) -> dict:\n",
    "        \"\"\"Track local model with carbon and performance metrics.\"\"\"\n",
    "        \n",
    "        # Initialize local model\n",
    "        local_llm = OllamaLLM(model=model_name)\n",
    "        \n",
    "        # Start carbon tracking\n",
    "        tracker = EmissionsTracker(project_name=\"local_model_monitoring\", log_level=\"error\")\n",
    "        tracker.start()\n",
    "        \n",
    "        # Run model and measure time\n",
    "        start_time = time.time()\n",
    "        response = local_llm.invoke(question)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Stop carbon tracking\n",
    "        emissions_kg = tracker.stop()\n",
    "        \n",
    "        # Estimate tokens (local models don't report exact counts)\n",
    "        estimated_input_tokens = len(question) // 4\n",
    "        estimated_output_tokens = len(response) // 4\n",
    "        estimated_total_tokens = estimated_input_tokens + estimated_output_tokens\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'time': elapsed,\n",
    "            'estimated_tokens': estimated_total_tokens,\n",
    "            'tokens_per_second': estimated_total_tokens / elapsed if elapsed > 0 else 0,\n",
    "            'carbon_kg': emissions_kg,\n",
    "            'carbon_mg': emissions_kg * 1_000_000 if emissions_kg else 0,\n",
    "            'cost': 0.0,  # No API cost!\n",
    "            'electricity_cost': (emissions_kg * 1_000) * 0.00012 if emissions_kg else 0,\n",
    "            'answer': response\n",
    "        }\n",
    "    \n",
    "    # Test with smallest model\n",
    "    query = \"Explain machine learning in one sentence.\"\n",
    "    \n",
    "    print(\"LOCAL MODEL CARBON TRACKING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Test three different sizes\n",
    "    models_to_test = [\n",
    "        (\"qwen3:0.6b\", \"522MB\", \"0.6 billion\", \"Ultra-lightweight - Fastest\"),\n",
    "        (\"qwen3:1.7b\", \"1.4GB\", \"1.7 billion\", \"Balanced - Good speed/quality\"),\n",
    "        (\"qwen3:4b\", \"2.5GB\", \"4 billion\", \"Higher quality - Slower\"),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing multiple Qwen3 model sizes...\\n\")\n",
    "    \n",
    "    for model_name, size, params, note in models_to_test:\n",
    "        try:\n",
    "            print(f\"Testing {model_name} ({size}, {params} parameters)\")\n",
    "            print(f\"  Note: {note}\")\n",
    "            \n",
    "            metrics = track_local_model(model_name, query)\n",
    "            results.append((model_name, size, params, metrics))\n",
    "            \n",
    "            print(f\"  SUCCESS!\")\n",
    "            print(f\"    Time: {metrics['time']:.2f}s\")\n",
    "            print(f\"    Carbon: {metrics['carbon_mg']:.2f}mg CO2\")\n",
    "            print(f\"    Speed: {metrics['tokens_per_second']:.1f} tokens/sec\")\n",
    "            print(f\"    Cost: ${metrics['electricity_cost']:.8f} (electricity only)\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"  SKIPPED: {error_msg[:80]}\")\n",
    "            \n",
    "            if \"not found\" in error_msg.lower():\n",
    "                print(f\"  >> To install: ollama pull {model_name}\")\n",
    "            elif \"connection\" in error_msg.lower():\n",
    "                print(f\"  >> Make sure Ollama is running: ollama serve\")\n",
    "            \n",
    "            print(f\"  >> This section is optional - continue to Tutorial 05 (Observability with LangSmith)!\\n\")\n",
    "    \n",
    "    # Display results\n",
    "    if results:\n",
    "        model_name, size, params, m = results[0]\n",
    "        print(\"=\"*70)\n",
    "        print(\"LOCAL MODEL RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nModel: {model_name} ({size})\")\n",
    "        print(f\"  Time: {m['time']:.2f}s\")\n",
    "        print(f\"  Carbon: {m['carbon_mg']:.2f}mg CO2\")\n",
    "        print(f\"  Speed: {m['tokens_per_second']:.1f} tokens/sec\")\n",
    "        print(f\"  Cost: ${m['electricity_cost']:.8f} (electricity)\")\n",
    "        print(f\"\\nResponse: {m['answer'][:150]}...\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LOCAL MODEL SETUP NEEDED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nTo try carbon tracking:\")\n",
    "        print(\"  1. Install Ollama: https://ollama.ai/\")\n",
    "        print(\"  2. Install integration: pip install langchain-ollama\")\n",
    "        print(\"  3. Pull model: ollama pull qwen3:0.6b\")\n",
    "        print(\"  4. Re-run this cell\")\n",
    "        print(\"\\nNote: This is optional - continue to Tutorial 05 (Observability with LangSmith)!\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"=\"*70)\n",
    "    print(\"OPTIONAL: Local Model Carbon Tracking\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThis section requires additional setup:\")\n",
    "    print(\"  1. Install langchain-ollama: pip install langchain-ollama\")\n",
    "    print(\"  2. Install Ollama: https://ollama.ai/\")\n",
    "    print(\"  3. Pull a model: ollama pull qwen3:0.6b\")\n",
    "    print(\"\\nThis is OPTIONAL - continue to Tutorial 05 (Observability with LangSmith)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 7: Cost Comparison - API vs Local\n",
    "\n",
    "### Understanding the Trade-offs\n",
    "\n",
    "**Cloud API (Claude 3.5 Sonnet (via Bedrock)):**\n",
    "- Input: $0.15 per 1M tokens\n",
    "- Output: $0.60 per 1M tokens\n",
    "- Typical query: ~$0.000001-0.000002\n",
    "- Scales linearly with usage\n",
    "- No setup required\n",
    "- Best quality\n",
    "\n",
    "**Local Models (Qwen3):**\n",
    "- Model cost: $0 (free download)\n",
    "- API cost: $0 (runs locally)\n",
    "- Only cost: Electricity (~$0.12/kWh in US)\n",
    "- Typical query: ~$0.0000001-0.0000005\n",
    "- Requires setup and hardware\n",
    "- Privacy-friendly\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use Cloud APIs when:**\n",
    "- Low volume (<1,000 queries/month)\n",
    "- Best quality needed\n",
    "- Rapid prototyping\n",
    "- No infrastructure available\n",
    "\n",
    "**Use Local Models when:**\n",
    "- High volume (10,000+ queries/month)\n",
    "- Privacy critical\n",
    "- Offline operation required\n",
    "- Cost sensitive\n",
    "\n",
    "### Real-World Scenarios\n",
    "\n",
    "**Scenario 1: Chatbot (100,000 queries/month)**\n",
    "- Claude 3.5 Sonnet (via Bedrock): ~$200/month\n",
    "- qwen3:0.6b: ~$0.05/month (electricity)\n",
    "- **Savings: $199.95/month**\n",
    "\n",
    "**Scenario 2: Low Volume (1,000 queries/month)**\n",
    "- Claude 3.5 Sonnet (via Bedrock): ~$2/month\n",
    "- qwen3:0.6b: ~$0.01/month (but setup time may not be worth it)\n",
    "- **Better to use API for convenience**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-part-a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered performance monitoring, cost tracking, and carbon emissions measurement.\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Latency Tracking** - Measure response times accurately\n",
    "2. **Token Counting** - Use tiktoken for precise token counts\n",
    "3. **Cost Estimation** - Calculate API costs based on actual token usage\n",
    "4. **Multi-Agent Comparison** - Compare different agent configurations\n",
    "5. **Real-World Tool Impact** - Understand how search tools affect token consumption\n",
    "6. **Carbon Tracking** - Measure emissions for local models\n",
    "7. **Cost Comparison** - API vs local model trade-offs\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- tiktoken provides accurate token counts for cost estimation\n",
    "- Claude 3.5 Sonnet (via Bedrock): $0.15/1M input, $0.60/1M output tokens\n",
    "- Search tools can increase token consumption by 20-40x\n",
    "- Local models cost $0 in API fees (only electricity)\n",
    "- Carbon tracking only makes sense for local models where you control hardware\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. **Monitor tool return sizes** - Limit search results, truncate API responses\n",
    "2. **Use tool-based routing** - Only call expensive tools when necessary\n",
    "3. **Implement caching** - Cache search results for common queries\n",
    "4. **Track real costs** - Monitor actual production usage over time\n",
    "5. **Optimize prompts** - Reduce tokens to lower costs\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Continue your learning journey:\n",
    "- **05_observability.ipynb** - Learn deep observability with LangSmith tracing\n",
    "- **06_benchmark_evaluation.ipynb** - Test agents on PhD-level questions\n",
    "- **Advanced**: Set up production monitoring dashboards\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Tiktoken Documentation](https://github.com/openai/tiktoken)\n",
    "- [CodeCarbon Documentation](https://mlco2.github.io/codecarbon/)\n",
    "- [OpenAI Pricing](https://openai.com/api/pricing/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}