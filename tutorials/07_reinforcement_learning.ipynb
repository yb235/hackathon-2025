{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"../assets/images/hackathon.png\" alt=\"Holistic AI Hackathon Logo\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Event**: [hackathon.holisticai.com](https://hackathon.holisticai.com)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Reinforcement Learning for Agents\n",
    "\n",
    "**Learn advanced RL concepts for agent training (Advanced/Optional)**\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: This is an **advanced, optional tutorial** focused on **conceptual learning**.  \n",
    "> **Practical RL training requires GPU resources and hours of training time**, which may not be feasible during a 48-hour hackathon.  \n",
    "> This tutorial teaches **concepts and methods** rather than requiring full implementation.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Understand** why RL can improve agent performance\n",
    "2. **Learn** the concepts behind RL training (trajectories, rewards, GRPO)\n",
    "3. **See** how RULER automates reward functions\n",
    "4. **Explore** training patterns (without full training setup)\n",
    "\n",
    "## Why This Tutorial?\n",
    "\n",
    "- **Educational**: Understand advanced agent training techniques\n",
    "- **Conceptual**: Learn RL concepts without GPU requirements\n",
    "- **Reference**: See how production RL training works\n",
    "- **Optional**: Not required for hackathon projects\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Recommended: Completed tutorials 01-03\n",
    "- Time: ~20 minutes (conceptual learning)\n",
    "- **Note**: Full RL training requires GPU (8GB+ VRAM) and hours of training time\n",
    "- **Holistic AI Bedrock API** (optional, for examples) - Credentials will be provided during the hackathon event\n",
    "\n",
    "**API Guide**: [../assets/api-guide.pdf](../assets/api-guide.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning? (Simple Explanation)\n",
    "\n",
    "**Think of training an AI agent like training a dog:**\n",
    "\n",
    "üêï **Traditional AI (Prompting)**: You give the dog a command, and it does its best based on what it already knows. It's like asking a well-trained dog to \"sit\" - it knows the command, but might not be perfect.\n",
    "\n",
    "üéì **Reinforcement Learning**: You let the dog try different things, reward it when it does well, and it learns from experience. Over time, the dog gets better and better at the task.\n",
    "\n",
    "### Real-World Analogy\n",
    "\n",
    "Imagine teaching a child to play chess:\n",
    "\n",
    "1. **Traditional Prompting**: You explain the rules once, and the child plays based on that explanation.\n",
    "   - ‚úÖ Fast to start\n",
    "   - ‚ùå Limited by initial knowledge\n",
    "   - ‚ùå Can't improve through practice\n",
    "\n",
    "2. **Reinforcement Learning**: The child plays many games, learns from wins and losses, and gets better over time.\n",
    "   - ‚úÖ Learns from experience\n",
    "   - ‚úÖ Gets better with practice\n",
    "   - ‚úÖ Can discover new strategies\n",
    "   - ‚è±Ô∏è Takes time to train\n",
    "\n",
    "### Key Concepts (In Simple Terms)\n",
    "\n",
    "**Agent**: The AI that learns (like the chess player)\n",
    "\n",
    "**Environment**: The task or problem (like the chess board)\n",
    "\n",
    "**Action**: What the agent does (like making a chess move)\n",
    "\n",
    "**Reward**: Feedback on how well the agent did (like winning or losing)\n",
    "\n",
    "**Training**: The process of learning from many attempts\n",
    "\n",
    "### Why Use RL for AI Agents?\n",
    "\n",
    "**Regular AI (Prompting)**:\n",
    "- Like reading a manual once\n",
    "- Works for common tasks\n",
    "- Can't improve without new instructions\n",
    "\n",
    "**RL-Trained AI**:\n",
    "- Like practicing a skill\n",
    "- Gets better with experience\n",
    "- Can handle complex, multi-step tasks\n",
    "- Learns optimal strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load from .env file in parent directory\nenv_path = Path('../.env')\nif env_path.exists():\n    load_dotenv(env_path)\n    print(\"üìÑ Loaded configuration from .env file\")\nelse:\n    print(\"‚ö†Ô∏è  No .env file found - using environment variables\")\n\n# Verify API keys\nprint(\"\\nüîë API Key Status:\")\nif os.getenv('HOLISTIC_AI_TEAM_ID') and os.getenv('HOLISTIC_AI_API_TOKEN'):\n    print(\"  ‚úÖ Holistic AI Bedrock credentials loaded\")\nelif os.getenv('OPENAI_API_KEY'):\n    print(\"  ‚ö†Ô∏è  OpenAI API key loaded\")\nelse:\n    print(\"  ‚ö†Ô∏è  No API keys found\")\n\nprint(\"\\nüìÅ Working directory:\", Path.cwd())\n\n# Import Holistic AI Bedrock helper\nimport sys\ntry:\n    sys.path.insert(0, '../core')\n    from react_agent.holistic_ai_bedrock import get_chat_model\n    print(\"\\n‚úÖ Holistic AI Bedrock helper loaded\")\nexcept ImportError:\n    print(\"\\n‚ö†Ô∏è  Could not import from core - will use OpenAI only\")\n\n# Import official packages\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage\nimport numpy as np\n\nprint(\"\\n‚úÖ All imports successful!\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important: Resource Requirements\n",
    "\n",
    "**This tutorial focuses on concepts, not full training.**\n",
    "\n",
    "### Full RL Training Requirements (Not Required Here)\n",
    "\n",
    "If you wanted to do **actual RL training**, you would need:\n",
    "\n",
    "- **GPU**: 8GB+ VRAM (e.g., RTX 3060) - **Most participants won't have this**\n",
    "- **Time**: 2-10 hours of training time - **Not feasible in 48-hour hackathon**\n",
    "- **Setup**: ART backend, model registration - **Complex configuration**\n",
    "\n",
    "### What This Tutorial Provides Instead\n",
    "\n",
    "‚úÖ **Conceptual understanding** - Learn how RL works  \n",
    "‚úÖ **Code patterns** - See training structure  \n",
    "‚úÖ **Evaluation methods** - Understand how to measure improvements  \n",
    "‚úÖ **Quick examples** - Simple demonstrations without GPU  \n",
    "\n",
    "**You can learn RL concepts without actually training!**\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Recommended: Completed tutorials 01-03\n",
    "- Time: ~20 minutes (conceptual learning)\n",
    "\n",
    "**Note**: This tutorial teaches concepts. Full RL training requires GPU resources and is not required for hackathon projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup (Conceptual Only)\n",
    "\n",
    "This tutorial focuses on concepts. We'll use simple examples that don't require GPU or full training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  No GPU detected!\n",
      "   RL training will be slow on CPU\n",
      "   Consider using Google Colab or cloud GPU\n",
      "\n",
      "‚úÖ Setup complete!\n",
      "‚úÖ Bedrock API helper loaded\n"
     ]
    }
   ],
   "source": [
    "# Simple setup - no GPU required for conceptual learning\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"\\nüìö This tutorial focuses on concepts.\")\n",
    "print(\"   No GPU or full training setup required.\")\n",
    "print(\"   We'll learn RL patterns through examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding RL for Agents\n",
    "\n",
    "### The Two Ways to Make AI Work\n",
    "\n",
    "Think of AI agents like employees. There are two ways to get them to do work:\n",
    "\n",
    "#### Method 1: Prompting (What We've Been Doing)\n",
    "\n",
    "**Like giving instructions to a new employee:**\n",
    "\n",
    "```python\n",
    "# You tell the AI what to do\n",
    "llm = get_chat_model('claude-3-5-sonnet')  # Uses Holistic AI Bedrock\n",
    "agent = create_react_agent(tools=[search], llm=llm)\n",
    "result = agent.invoke(\"Find info about quantum computing\")\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "- You write clear instructions (prompts)\n",
    "- The AI follows those instructions\n",
    "- It uses its general knowledge\n",
    "- Works immediately, no training needed\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Fast to set up (minutes)\n",
    "- ‚úÖ No training needed\n",
    "- ‚úÖ Works for many tasks\n",
    "- ‚úÖ Easy to change instructions\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Limited by the AI's general knowledge\n",
    "- ‚ùå May fail on complex, specific tasks\n",
    "- ‚ùå Can't improve without new prompts\n",
    "- ‚ùå Not specialized for your specific needs\n",
    "\n",
    "**Real Example**: Like asking a general-purpose assistant to help with your specific workflow. They understand, but might not be perfect.\n",
    "\n",
    "#### Method 2: RL Training (What We'll Learn)\n",
    "\n",
    "**Like training an employee through practice:**\n",
    "\n",
    "```python\n",
    "# You let the AI practice and learn\n",
    "trained_agent = art.TrainableModel(\n",
    "    base_model='unsloth/Llama-3.2-3B-Instruct'\n",
    ")\n",
    "await trained_agent.train(trajectory_groups)  # Learn from experience\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "- The AI tries many different approaches\n",
    "- It gets feedback on what works (rewards)\n",
    "- It learns from successes and failures\n",
    "- It gets better over time\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Specialized for your specific task\n",
    "- ‚úÖ More reliable and consistent\n",
    "- ‚úÖ Better at complex, multi-step tasks\n",
    "- ‚úÖ Can discover optimal strategies\n",
    "- ‚úÖ Improves with more training\n",
    "\n",
    "**Cons:**\n",
    "- ‚è±Ô∏è Takes time to train (hours)\n",
    "- üíª Needs GPU resources\n",
    "- üìä Requires training data/scenarios\n",
    "- üîß More complex setup\n",
    "\n",
    "**Real Example**: Like training a specialist who practices your specific workflow until they become an expert.\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Traditional Prompting:\n",
    "You ‚Üí Instructions ‚Üí AI ‚Üí Response\n",
    "(One-time, based on general knowledge)\n",
    "\n",
    "RL Training:\n",
    "You ‚Üí Scenarios ‚Üí AI tries ‚Üí Gets feedback ‚Üí Learns ‚Üí Tries again ‚Üí Better!\n",
    "(Repeated practice, learns from experience)\n",
    "```\n",
    "\n",
    "### When Should You Use Each Method?\n",
    "\n",
    "**Use Traditional Prompting When:**\n",
    "- ‚úÖ Task is simple and straightforward\n",
    "- ‚úÖ You need results quickly\n",
    "- ‚úÖ Task changes frequently\n",
    "- ‚úÖ 70-80% success rate is acceptable\n",
    "- ‚úÖ You don't have GPU resources\n",
    "\n",
    "**Use RL Training When:**\n",
    "- ‚úÖ Task requires 90%+ success rate\n",
    "- ‚úÖ Complex, multi-step interactions\n",
    "- ‚úÖ Task is repetitive and well-defined\n",
    "- ‚úÖ You have training scenarios or can generate them\n",
    "- ‚úÖ You need specialized performance\n",
    "- ‚úÖ You have GPU resources available\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "**Traditional Prompting Works Well For:**\n",
    "- Answering general questions\n",
    "- Simple one-step tasks\n",
    "- Creative writing\n",
    "- Quick prototypes\n",
    "\n",
    "**RL Training Works Better For:**\n",
    "- Email management agents (learn your specific workflow)\n",
    "- Game-playing agents (learn strategies)\n",
    "- Customer support (learn to handle complex cases)\n",
    "- Research agents (learn to use tools effectively)\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "**Prompting** = Fast, general-purpose, good enough for many tasks\n",
    "\n",
    "**RL Training** = Slower, specialized, excellent for specific tasks\n",
    "\n",
    "**In this tutorial**, you'll learn how to use RL training to create highly specialized AI agents that excel at specific tasks! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ART Architecture Overview\n",
    "\n",
    "### What is ART? (Simple Explanation)\n",
    "\n",
    "**ART (Agent Reinforcement Trainer)** is like a training gym for AI agents:\n",
    "\n",
    "- üèãÔ∏è **The Gym (ART Backend)**: Where the actual training happens\n",
    "- üë§ **The Trainer (Your Code)**: You define what the agent should learn\n",
    "- üéØ **The Student (AI Agent)**: The AI that gets trained\n",
    "- üèÜ **The Scoreboard (RULER)**: Automatically judges how well the agent did\n",
    "\n",
    "### The Training Process (Step by Step)\n",
    "\n",
    "Think of it like training for a sport:\n",
    "\n",
    "1. **You provide scenarios** (like practice drills)\n",
    "   - \"Answer this question\"\n",
    "   - \"Solve this problem\"\n",
    "   - \"Complete this task\"\n",
    "\n",
    "2. **The agent tries multiple times** (like practicing a move)\n",
    "   - Each attempt is called a \"rollout\"\n",
    "   - Agent tries different approaches\n",
    "   - Creates multiple solutions\n",
    "\n",
    "3. **RULER judges the attempts** (like a coach scoring performance)\n",
    "   - Compares all attempts\n",
    "   - Ranks them from best to worst\n",
    "   - Assigns scores automatically\n",
    "\n",
    "4. **The agent learns** (like improving through practice)\n",
    "   - Sees what worked well\n",
    "   - Adjusts its strategy\n",
    "   - Gets better over time\n",
    "\n",
    "5. **Repeat** (like training sessions)\n",
    "   - More scenarios = More practice\n",
    "   - More training steps = More improvement\n",
    "\n",
    "### ART Architecture (Technical View)\n",
    "\n",
    "ART (Agent Reinforcement Trainer) has a client-server architecture:\n",
    "\n",
    "```\n",
    "                      Your Codebase                          \n",
    "\n",
    "    ART Client (Frontend)                                 \n",
    "    - Minimal dependencies                                \n",
    "    - Wraps your agent logic                             \n",
    "    - Collects trajectories                              \n",
    "\n",
    "\n",
    "                          \n",
    "                           API\n",
    "                          \n",
    "\n",
    "              ART Backend (Training Server)                  \n",
    "\n",
    "    - Unsloth GRPO Trainer                                \n",
    "    - GPU acceleration                                    \n",
    "    - Model serving (OpenAI-compatible)                  \n",
    "    - RULER reward function                              \n",
    "```\n",
    "\n",
    "### Key Components Explained Simply\n",
    "\n",
    "1. **Trajectory** - A record of what the agent did\n",
    "   - Like a game replay showing all moves\n",
    "   - Contains: question, agent's response, reward score\n",
    "\n",
    "2. **Rollout Function** - How your agent tries the task\n",
    "   - Like a practice attempt\n",
    "   - Agent generates a response\n",
    "   - Records what happened\n",
    "\n",
    "3. **Reward Function (RULER)** - Scores how well the agent did\n",
    "   - Like a judge giving scores\n",
    "   - Compares multiple attempts\n",
    "   - Automatically assigns scores (0.0 to 1.0)\n",
    "\n",
    "4. **Training Loop** - The learning process\n",
    "   - Collects many trajectories\n",
    "   - Scores them with RULER\n",
    "   - Updates the agent's \"brain\" (model weights)\n",
    "   - Repeats to improve\n",
    "\n",
    "### Why This Architecture?\n",
    "\n",
    "**Separation of Concerns:**\n",
    "- Your code focuses on defining the task\n",
    "- ART backend handles the complex training\n",
    "- You don't need to understand all the math!\n",
    "\n",
    "**Scalability:**\n",
    "- Backend can run on powerful GPUs\n",
    "- Your code can run anywhere\n",
    "- Easy to scale up training\n",
    "\n",
    "**Flexibility:**\n",
    "- Works with different models\n",
    "- Supports various tasks\n",
    "- Easy to experiment\n",
    "\n",
    "ART (Agent Reinforcement Trainer) has a client-server architecture:\n",
    "\n",
    "```\n",
    "\n",
    "                      Your Codebase                          \n",
    "    \n",
    "    ART Client (Frontend)                                 \n",
    "    - Minimal dependencies                                \n",
    "    - Wraps your agent logic                             \n",
    "    - Collects trajectories                              \n",
    "    \n",
    "\n",
    "                          \n",
    "                           API\n",
    "                          \n",
    "\n",
    "              ART Backend (Training Server)                  \n",
    "    \n",
    "    - Unsloth GRPO Trainer                                \n",
    "    - GPU acceleration                                    \n",
    "    - Model serving (OpenAI-compatible)                  \n",
    "    - RULER reward function                              \n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Trajectory** - Record of agent's actions and environment responses\n",
    "2. **Rollout Function** - How your agent interacts with the environment\n",
    "3. **Reward Function** - Scores how well the agent did (RULER automates this!)\n",
    "4. **Training Loop** - GRPO updates the model based on rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Supported Models for RL Training\n",
    "\n",
    "ART + Unsloth support many models for RL training. Here are the main options:\n",
    "\n",
    "### Small Models (Recommended for Limited Resources) üöÄ\n",
    "\n",
    "**Best for: Low VRAM (4-8GB), Fast Training, Quick Iteration**\n",
    "\n",
    "- **Llama 3.2 1B** - `unsloth/Llama-3.2-1B-Instruct` (1B params, ~4GB VRAM)\n",
    "- **Llama 3.2 3B** - `unsloth/Llama-3.2-3B-Instruct` (3B params, ~6GB VRAM)\n",
    "- **Qwen2.5 Coder 1.5B** - `unsloth/Qwen2.5-Coder-1.5B-Instruct` (1.5B params, ~4GB VRAM)\n",
    "- **Qwen3 4B** - `unsloth/Qwen3-4B-Instruct` (4B params, ~8GB VRAM)\n",
    "- **Phi-3.5 Mini** - `unsloth/Phi-3.5-Mini-Instruct` (~3.8B params, ~6GB VRAM)\n",
    "- **Phi-4** - `unsloth/Phi-4` (Latest Phi model)\n",
    "\n",
    "### Medium Models (Balanced Performance)\n",
    "\n",
    "**Best for: Moderate VRAM (8-16GB), Better Quality**\n",
    "\n",
    "- **Qwen3-8B** - `unsloth/Qwen3-8B-Instruct` (8B params, ~12GB VRAM)\n",
    "- **Qwen2.5-7B** - `unsloth/Qwen2.5-7B-Instruct` (7B params, ~10GB VRAM)\n",
    "- **Llama 3.1 8B** - `unsloth/Llama-3.1-8B-Instruct-bnb-4bit` (8B params, ~10GB VRAM)\n",
    "- **Mistral 7B** - `unsloth/Mistral-7B-Instruct-v0.3-bnb-4bit` (7B params, ~10GB VRAM)\n",
    "\n",
    "### Large Models (Maximum Performance)\n",
    "\n",
    "**Best for: High VRAM (16GB+), Production Quality**\n",
    "\n",
    "- **Qwen3-14B** - `unsloth/Qwen3-14B-Instruct` (14B params, ~18GB VRAM)\n",
    "- **Qwen2.5-14B** - `unsloth/Qwen2.5-14B-Instruct` (14B params, ~18GB VRAM)\n",
    "- **Llama 3.1 70B** - `unsloth/Llama-3.1-70B-Instruct-bnb-4bit` (70B params, ~40GB VRAM)\n",
    "\n",
    "### Model Selection Guide\n",
    "\n",
    "**Choose Small Models (1-4B) if:**\n",
    "- ‚úÖ Limited GPU memory (4-8GB VRAM)\n",
    "- ‚úÖ Need fast training iterations\n",
    "- ‚úÖ Testing/prototyping\n",
    "- ‚úÖ Simple tasks\n",
    "\n",
    "**Choose Medium Models (7-8B) if:**\n",
    "- ‚úÖ Moderate GPU (8-16GB VRAM)\n",
    "- ‚úÖ Need better quality\n",
    "- ‚úÖ Complex tasks\n",
    "- ‚úÖ Production deployment\n",
    "\n",
    "**Choose Large Models (14B+) if:**\n",
    "- ‚úÖ High-end GPU (16GB+ VRAM)\n",
    "- ‚úÖ Maximum quality needed\n",
    "- ‚úÖ Research/competition\n",
    "\n",
    "**Note**: All models support 4-bit quantization (`-bnb-4bit` suffix) for memory efficiency.\n",
    "\n",
    "**For Judge Models** (RULER scoring):\n",
    "- Use **Holistic AI Bedrock API** (recommended): `claude-3-5-sonnet`, `claude-3-5-haiku`\n",
    "- Or OpenAI: `gpt-5-mini`, `gpt-4-turbo`\n",
    "- Or local models via Ollama\n",
    "\n",
    "**GPU Requirements**:\n",
    "- 1-3B models: 4-6GB VRAM\n",
    "- 4-8B models: 8-12GB VRAM\n",
    "- 14B models: 16-20GB VRAM\n",
    "- 70B models: 40GB+ VRAM (or use quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simple Example - Math Problem Solver\n",
    "\n",
    "Let's train an agent to solve math problems. This is a simpler example to understand the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conceptual Example: Training a Math Agent\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1‚É£  SETUP\n",
      "   model = art.TrainableModel(\n",
      "       name='math-agent',\n",
      "       base_model='unsloth/Llama-3.2-3B-Instruct'  # Small model (~6GB VRAM). Or Qwen3-4B, Qwen3-14B, etc.\n",
      "   )\n",
      "\n",
      "2‚É£  ROLLOUT FUNCTION\n",
      "   async def rollout(model, problem):\n",
      "       # Agent tries to solve the problem\n",
      "       response = await model.generate(problem)\n",
      "       return trajectory  # Record of what happened\n",
      "\n",
      "3‚É£  REWARD\n",
      "   # Option A: Manual reward\n",
      "   reward = 1.0 if answer_correct else 0.0\n",
      "   \n",
      "   # Option B: RULER (automatic!)\n",
      "   reward = await ruler_score(trajectory, judge_model='openai/o3')\n",
      "\n",
      "4‚É£  TRAINING\n",
      "   await model.train(trajectory_groups)\n",
      "   # Model learns: 'What actions led to good rewards?'\n",
      "\n",
      "======================================================================\n",
      "\n",
      " The agent improves by trying many problems,\n",
      "   getting rewards, and learning what works!\n"
     ]
    }
   ],
   "source": [
    "# Simplified conceptual example (not executable without full ART setup)\n",
    "# This shows the pattern you'll use\n",
    "\n",
    "print(\" Conceptual Example: Training a Math Agent\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Define what you want to train\n",
    "print(\"\\n1‚É£  SETUP\")\n",
    "print(\"   model = art.TrainableModel(\")\n",
    "print(\"       name='math-agent',\")\n",
    "print(\"       base_model='unsloth/Llama-3.2-3B-Instruct'  # Small model (~6GB VRAM). Or Qwen3-4B, Qwen3-14B, etc.\")\n",
    "print(\"   )\")\n",
    "\n",
    "# 2. Define how the agent works\n",
    "print(\"\\n2‚É£  ROLLOUT FUNCTION\")\n",
    "print(\"   async def rollout(model, problem):\")\n",
    "print(\"       # Agent tries to solve the problem\")\n",
    "print(\"       response = await model.generate(problem)\")\n",
    "print(\"       return trajectory  # Record of what happened\")\n",
    "\n",
    "# 3. Score the attempts\n",
    "print(\"\\n3‚É£  REWARD\")\n",
    "print(\"   # Option A: Manual reward\")\n",
    "print(\"   reward = 1.0 if answer_correct else 0.0\")\n",
    "print(\"   \")\n",
    "print(\"   # Option B: RULER (automatic!)\")\n",
    "print(\"   reward = await ruler_score(trajectory, judge_model='openai/o3')\")\n",
    "\n",
    "# 4. Train!\n",
    "print(\"\\n4‚É£  TRAINING\")\n",
    "print(\"   await model.train(trajectory_groups)\")\n",
    "print(\"   # Model learns: 'What actions led to good rewards?'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n The agent improves by trying many problems,\")\n",
    "print(\"   getting rewards, and learning what works!\")\n",
    "\n",
    "# Run training\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Training Scenarios\n",
    "\n",
    "Define scenarios to train your agent on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 3 training scenarios\n",
      "\n",
      " Sample:\n",
      "   Q: What is 2 + 2?\n",
      "   A: 4\n",
      "   Difficulty: easy\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TrainingScenario(BaseModel):\n",
    "    \"\"\"A single training scenario.\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    difficulty: str = \"medium\"  # easy, medium, hard\n",
    "    subject: Optional[str] = None\n",
    "\n",
    "# Load HLE dataset for realistic training scenarios\n",
    "print(\"Loading HLE dataset for training scenarios...\")\n",
    "hle_dataset = load_dataset('cais/hle', split='test')\n",
    "\n",
    "# Select diverse HLE questions for training\n",
    "# Choose questions from different subjects and difficulty levels\n",
    "selected_indices = [0, 100, 500, 1000, 1500]  # Diverse samples\n",
    "training_scenarios = []\n",
    "\n",
    "for idx in selected_indices:\n",
    "    if idx < len(hle_dataset):\n",
    "        sample = hle_dataset[idx]\n",
    "        scenario = TrainingScenario(\n",
    "            id=f\"hle_{idx:04d}\",\n",
    "            question=sample.get('question', ''),\n",
    "            expected_answer=sample.get('answer', ''),\n",
    "            difficulty=\"hard\",  # HLE questions are PhD-level\n",
    "            subject=sample.get('raw_subject', 'Unknown')\n",
    "        )\n",
    "        training_scenarios.append(scenario)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(training_scenarios)} HLE training scenarios\")\n",
    "print(f\"\\n Sample scenarios:\")\n",
    "for s in training_scenarios[:3]:\n",
    "    print(f\"   [{s.subject}] {s.question[:60]}...\")\n",
    "    print(f\"      Expected: {s.expected_answer[:40]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Complete ART Template\n",
    "\n",
    "Here's a complete template you can adapt for your use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete ART Training Template\n",
      "\n",
      "This is a reference template. Adapt for your specific task!\n",
      "======================================================================\n",
      "\n",
      "import art\n",
      "from art.rewards import ruler_score_group\n",
      "\n",
      "# 1. Initialize trainable model\n",
      "model = art.TrainableModel(\n",
      "    name=\"my-agent\",\n",
      "    project=\"my-project\",\n",
      "    base_model=\"unsloth/Llama-3.2-3B-Instruct\",  # Small model (~6GB). Or Qwen3-4B, Qwen3-14B, Llama-3.1-8B, etc.\n",
      ")\n",
      "\n",
      "# 2. Define your scenarios (what to train on)\n",
      "scenarios = [\n",
      "    {\"task\": \"Find information about quantum computing\"},\n",
      "    {\"task\": \"Solve: What is 2+2?\"},\n",
      "    # Add more scenarios...\n",
      "]\n",
      "\n",
      "# 3. Define rollout function (how agent interacts)\n",
      "async def rollout(model: art.Model, scenario: dict) -> art.Trajectory:\n",
      "    # Get OpenAI-compatible client\n",
      "    client = model.openai_client()\n",
      "\n",
      "    # Create trajectory\n",
      "    trajectory = art.Trajectory(\n",
      "        messages_and_choices=[\n",
      "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "            {\"role\": \"user\", \"content\": scenario[\"task\"]}\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    # Agent takes actions (can be multi-turn!)\n",
      "    response = await client.chat.completions.create(\n",
      "        model=model.name,\n",
      "        messages=trajectory.messages_and_choices\n",
      "    )\n",
      "\n",
      "    # Add response to trajectory\n",
      "    trajectory.add_assistant_message(response.choices[0].message.content)\n",
      "\n",
      "    return trajectory\n",
      "\n",
      "# 4. Gather trajectories and score with RULER\n",
      "groups = await art.gather_trajectory_groups(\n",
      "    (\n",
      "        art.TrajectoryGroup([rollout(model, scenario) for _ in range(8)])\n",
      "        for scenario in scenarios\n",
      "    ),\n",
      "    after_each=lambda group: ruler_score_group(\n",
      "        group,\n",
      "        \"openai/gpt-5-mini\",  # Judge model\n",
      "        swallow_exceptions=True\n",
      "    )\n",
      ")\n",
      "\n",
      "# 5. Train!\n",
      "await model.train(groups)\n",
      "\n",
      "# 6. Use the trained model\n",
      "trained_agent = model.openai_client()\n",
      "result = await trained_agent.chat.completions.create(\n",
      "    model=model.name,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"New task here\"}]\n",
      ")\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Adapt this template for your specific agent task!\n"
     ]
    }
   ],
   "source": [
    "# Training Loop - Following tic_tac_toe Pattern\n",
    "\n",
    "TRAINING_STEPS = 2\n",
    "ROLLOUTS_PER_STEP = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üöÄ Starting Training Loop\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training steps: {TRAINING_STEPS}\")\n",
    "print(f\"Rollouts per step: {ROLLOUTS_PER_STEP}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Training loop (following tic_tac_toe pattern)\n",
    "for i in range(await model.get_step(), TRAINING_STEPS):\n",
    "    print(f\"\\nüìä Step {i + 1}/{TRAINING_STEPS}\")\n",
    "    \n",
    "    # Gather trajectory groups with RULER scoring\n",
    "    train_groups = await art.gather_trajectory_groups(\n",
    "        (\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(model, scenario)\n",
    "                for _ in range(ROLLOUTS_PER_STEP)\n",
    "            )\n",
    "            for scenario in training_scenarios\n",
    "        ),\n",
    "        after_each=lambda group: ruler_score_group(\n",
    "            group,\n",
    "            \"openai/gpt-5-mini\",  # Judge model for RULER\n",
    "            swallow_exceptions=True\n",
    "        ),\n",
    "        pbar_desc=\"gather\",\n",
    "    )\n",
    "    \n",
    "    # Delete old checkpoints (like tic_tac_toe)\n",
    "    await model.delete_checkpoints()\n",
    "    \n",
    "    # Train with config (like tic_tac_toe)\n",
    "    print(f\"\\nüîÑ Training model (step {i + 1})...\")\n",
    "    await model.train(\n",
    "        train_groups,\n",
    "        config=art.TrainConfig(learning_rate=LEARNING_RATE)\n",
    "    )\n",
    "    print(f\"‚úÖ Step {i + 1} complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nModel weights have been updated via GRPO!\")\n",
    "print(\"The model should now perform better on HLE questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: Key RL Concepts for Agents\n",
    "\n",
    "Before diving into training, understand these critical concepts:\n",
    "\n",
    "### 1. Trajectory Diversity\n",
    "\n",
    "**Why it matters**: Multiple rollouts per scenario (`rollouts_per_group`) create diverse trajectories, allowing RULER to compare and rank them.\n",
    "\n",
    "- **More rollouts** = Better comparison = More reliable rewards\n",
    "- **Recommended**: 4-8 rollouts per scenario\n",
    "- **Trade-off**: More rollouts = Slower training but better quality\n",
    "\n",
    "```python\n",
    "# Good: Multiple diverse trajectories\n",
    "art.TrajectoryGroup(\n",
    "    rollout(model, scenario) for _ in range(8)  # 8 diverse attempts\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. RULER Reward Function\n",
    "\n",
    "**RULER (Reward Using LLM Evaluation and Ranking)** automatically scores trajectories:\n",
    "\n",
    "- Compares multiple trajectories for the same scenario\n",
    "- Uses a judge model (e.g., `claude-3-5-sonnet`, `gpt-5-mini`) to rank them\n",
    "- Assigns relative scores (0.0-1.0) based on quality\n",
    "- **No manual reward engineering needed!**\n",
    "\n",
    "**Best Practices**:\n",
    "- Use `swallow_exceptions=True` for robust error handling\n",
    "- Choose a strong judge model (API models recommended)\n",
    "- Ensure judge model understands your task domain\n",
    "\n",
    "```python\n",
    "ruler_score_group(\n",
    "    group,\n",
    "    \"openai/gpt-5-mini\",  # Strong judge model\n",
    "    swallow_exceptions=True  # Handle errors gracefully\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Exploration vs Exploitation\n",
    "\n",
    "**Exploration**: Agent tries different approaches (high temperature, diverse rollouts)\n",
    "**Exploitation**: Agent uses learned knowledge (lower temperature, focused responses)\n",
    "\n",
    "- **During Training**: Balance exploration (to find good strategies) and exploitation (to refine them)\n",
    "- **Rollout Temperature**: Start higher (0.7-0.9) for exploration, lower (0.3-0.5) for exploitation\n",
    "- **Multiple Rollouts**: Natural exploration mechanism\n",
    "\n",
    "### 4. Training Stability\n",
    "\n",
    "**Key Techniques**:\n",
    "\n",
    "1. **Delete Checkpoints**: Clear old checkpoints before training\n",
    "   ```python\n",
    "   await model.delete_checkpoints()\n",
    "   ```\n",
    "\n",
    "2. **Learning Rate**: Start with 5e-5, adjust based on convergence\n",
    "   - Too high: Unstable training, loss spikes\n",
    "   - Too low: Slow convergence\n",
    "\n",
    "3. **Training Steps**: Monitor for convergence\n",
    "   - Early stopping if rewards plateau\n",
    "   - Continue if rewards still improving\n",
    "\n",
    "### 5. Common Pitfalls & Solutions\n",
    "\n",
    "| Problem | Cause | Solution |\n",
    "|---------|-------|----------|\n",
    "| **Reward Hacking** | Model finds shortcuts | Use diverse scenarios, check outputs manually |\n",
    "| **Low Rewards** | Poor judge model or scenarios | Use stronger judge, verify scenario quality |\n",
    "| **Training Slow** | Too many rollouts/scenarios | Reduce rollouts, use smaller model |\n",
    "| **No Improvement** | Learning rate too low/high | Tune learning rate, check reward distribution |\n",
    "| **GPU OOM** | Model too large | Use smaller model, reduce batch size |\n",
    "\n",
    "### 6. Multi-Turn Interactions\n",
    "\n",
    "**For complex tasks**, allow multiple turns:\n",
    "\n",
    "```python\n",
    "async def rollout(scenario: dict) -> art.Trajectory:\n",
    "    trajectory = art.Trajectory(messages_and_choices=[...])\n",
    "    \n",
    "    # Multi-turn loop\n",
    "    for turn in range(MAX_TURNS):\n",
    "        response = await client.chat.completions.create(...)\n",
    "        trajectory.add_assistant_message(response.content)\n",
    "        \n",
    "        # Check if task is complete\n",
    "        if task_complete(response):\n",
    "            break\n",
    "        \n",
    "        # Add follow-up message\n",
    "        trajectory.add_user_message(\"Continue...\")\n",
    "    \n",
    "    return trajectory\n",
    "```\n",
    "\n",
    "### 7. Tool Usage in RL\n",
    "\n",
    "**Training agents to use tools effectively**:\n",
    "\n",
    "- Include tool calls in trajectories\n",
    "- RULER judges tool usage quality\n",
    "- Model learns when and how to use tools\n",
    "\n",
    "**Example**: Agent learns to call search tool before answering questions\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Apply these concepts in Step 6 (Real Training)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Advanced Training Tips\n",
    "\n",
    "For production RL training, consider:\n",
    "\n",
    "### Training Best Practices\n",
    "\n",
    "1. **Start Small**\n",
    "   - Begin with 5-10 scenarios\n",
    "   - Use 2-4 rollouts per scenario initially\n",
    "   - Test the pipeline before scaling up\n",
    "   - Verify RULER scoring works correctly\n",
    "\n",
    "2. **Monitor Training**\n",
    "   - Watch reward scores improve over steps\n",
    "   - Check for reward hacking (model finds shortcuts)\n",
    "   - Validate on held-out scenarios\n",
    "   - Monitor GPU memory usage\n",
    "\n",
    "3. **Iterate Quickly**\n",
    "   - Add more scenarios gradually\n",
    "   - Tune learning rate (typically 1e-5 to 5e-5)\n",
    "   - Adjust rollout count based on GPU memory\n",
    "   - Use early stopping if rewards plateau\n",
    "\n",
    "4. **Evaluate Regularly**\n",
    "   - Test trained model on new scenarios\n",
    "   - Compare to baseline (prompted model)\n",
    "   - Use HLE benchmark (Tutorial 6) for evaluation\n",
    "   - Check for overfitting (good on train, bad on test)\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "- **Learning Rate**: 1e-5 to 5e-5 (start with 5e-5)\n",
    "  - Too high: Unstable, loss spikes\n",
    "  - Too low: Slow convergence\n",
    "  - Adjust based on reward improvement rate\n",
    "\n",
    "- **Rollouts per Scenario**: 4-8 (more = better but slower)\n",
    "  - More rollouts = Better RULER comparison\n",
    "  - Trade-off: Quality vs Speed\n",
    "  - Start with 4, increase if needed\n",
    "\n",
    "- **Training Steps**: 2-10 (depends on convergence)\n",
    "  - Monitor reward improvement\n",
    "  - Stop if rewards plateau\n",
    "  - Continue if still improving\n",
    "\n",
    "- **Temperature**: 0.7-0.9 during training (exploration)\n",
    "  - Higher = More diverse trajectories\n",
    "  - Lower = More focused responses\n",
    "  - Balance exploration vs exploitation\n",
    "\n",
    "- **Batch Size**: Auto-configured by ART (usually optimal)\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Low Rewards**: \n",
    "- Increase rollouts per scenario (better RULER comparison)\n",
    "- Check RULER judge model quality (use stronger model)\n",
    "- Verify scenarios are appropriate and clear\n",
    "- Check if judge model understands your task domain\n",
    "\n",
    "**Training Slow**:\n",
    "- Reduce rollouts per scenario\n",
    "- Use smaller base model\n",
    "- Check GPU utilization (should be high)\n",
    "- Reduce number of scenarios per batch\n",
    "\n",
    "**Model Not Improving**:\n",
    "- Increase training steps (may need more iterations)\n",
    "- Adjust learning rate (try 1e-5 or 2e-5)\n",
    "- Verify reward function (RULER) is working correctly\n",
    "- Check if scenarios are too diverse (may need more focused set)\n",
    "\n",
    "**Reward Hacking**:\n",
    "- Model finds shortcuts instead of solving task\n",
    "- **Solution**: Add more diverse scenarios, manually check outputs\n",
    "- Use stronger judge model\n",
    "- Add explicit constraints in system prompt\n",
    "\n",
    "**GPU Out of Memory**:\n",
    "- Use smaller model (1-3B instead of 7-14B)\n",
    "- Reduce batch size (if configurable)\n",
    "- Use gradient checkpointing\n",
    "- Reduce sequence length\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Track these metrics to measure improvement:\n",
    "\n",
    "1. **Average Reward**: Should increase over training steps\n",
    "2. **Success Rate**: Percentage of scenarios solved correctly\n",
    "3. **Tool Usage Accuracy**: If using tools, how often used correctly\n",
    "4. **Response Quality**: Manual evaluation of outputs\n",
    "5. **Baseline Comparison**: Compare to prompted model performance\n",
    "\n",
    "### Next Steps After Training\n",
    "\n",
    "1. **Save Model**: Trained model is automatically saved by ART backend\n",
    "2. **Test on New Scenarios**: Verify generalization\n",
    "3. **Deploy**: Use `model.openai_client()` for inference\n",
    "4. **Iterate**: Add more scenarios, retrain if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluating Trained Agents\n",
    "\n",
    "How do you know if RL training worked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real ART Training - Complete Runnable Example\n",
    "# Following tic_tac_toe pattern with backend options\n",
    "\n",
    "import art\n",
    "from art.rewards import ruler_score_group\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import asyncio\n",
    "import random\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv(Path('../.env'))\n",
    "\n",
    "# Choose backend based on availability\n",
    "OPENPIPE_API_KEY = os.getenv('OPENPIPE_API_KEY')\n",
    "\n",
    "if OPENPIPE_API_KEY:\n",
    "    print(\"‚úÖ Using OpenPipe Cloud Backend (no GPU needed)\")\n",
    "    backend = None  # Cloud backend auto-detected\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using LocalBackend (requires NVIDIA GPU)\")\n",
    "    print(\"   For macOS without GPU, set OPENPIPE_API_KEY in .env\")\n",
    "    try:\n",
    "        from art.local import LocalBackend\n",
    "        backend = LocalBackend(path=\"./.art\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå LocalBackend not available\")\n",
    "        print(\"   Install: pip install openpipe-art[backend]\")\n",
    "        backend = None\n",
    "\n",
    "# Load HLE scenarios\n",
    "hle_dataset = load_dataset('cais/hle', split='test')\n",
    "selected_indices = [0, 100, 500]\n",
    "training_scenarios = []\n",
    "\n",
    "for idx in selected_indices:\n",
    "    if idx < len(hle_dataset):\n",
    "        sample = hle_dataset[idx]\n",
    "        training_scenarios.append({\n",
    "            'id': f\"hle_{idx:04d}\",\n",
    "            'question': sample.get('question', ''),\n",
    "            'expected_answer': sample.get('answer', ''),\n",
    "            'subject': sample.get('raw_subject', 'Unknown')\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(training_scenarios)} HLE scenarios\")\n",
    "\n",
    "# Initialize model\n",
    "random.seed(42)\n",
    "model = art.TrainableModel(\n",
    "    name=\"hle-agent\",\n",
    "    project=\"hackathon-rl-training\",\n",
    "    base_model=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainableModel created\")\n",
    "print(f\"   Name: {model.name}\")\n",
    "print(f\"   Project: {model.project}\")\n",
    "print(f\"   Base model: unsloth/Llama-3.2-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Model and Define Rollout Function\n",
    "# Following tic_tac_toe pattern\n",
    "\n",
    "async def register_and_train():\n",
    "    # Register model\n",
    "    print(\"\\n1. Registering model...\")\n",
    "    try:\n",
    "        if backend:\n",
    "            await model.register(backend)\n",
    "        else:\n",
    "            # Cloud backend (auto-detected from OPENPIPE_API_KEY)\n",
    "            await model.register()\n",
    "        print(\"‚úÖ Model registered!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Registration failed: {e}\")\n",
    "        print(\"\\nüí° Backend Setup:\")\n",
    "        if not OPENPIPE_API_KEY:\n",
    "            print(\"   Option 1: Set OPENPIPE_API_KEY in .env (recommended for macOS)\")\n",
    "            print(\"   Option 2: Use LocalBackend with NVIDIA GPU\")\n",
    "        return\n",
    "    \n",
    "    # Define rollout function (following tic_tac_toe pattern)\n",
    "    async def rollout(scenario: dict) -> art.Trajectory:\n",
    "        trajectory = art.Trajectory(\n",
    "            messages_and_choices=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are answering a question from Humanity's Last Exam (HLE), a PhD-level academic benchmark. Provide your best answer.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": scenario['question']\n",
    "                }\n",
    "            ],\n",
    "            reward=0,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Use model's inference API (like tic_tac_toe)\n",
    "            client = AsyncOpenAI(\n",
    "                base_url=model.inference_base_url,\n",
    "                api_key=model.inference_api_key,\n",
    "            )\n",
    "            \n",
    "            chat_completion = await client.chat.completions.create(\n",
    "                model=model.get_inference_name(),\n",
    "                messages=trajectory.messages(),\n",
    "                max_completion_tokens=512,\n",
    "            )\n",
    "            \n",
    "            choice = chat_completion.choices[0]\n",
    "            content = choice.message.content\n",
    "            assert isinstance(content, str)\n",
    "            trajectory.messages_and_choices.append(choice)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Rollout error: {e}\")\n",
    "            trajectory.reward = -1\n",
    "        \n",
    "        return trajectory\n",
    "    \n",
    "    # Training loop (following tic_tac_toe pattern)\n",
    "    print(\"\\n2. Gathering trajectory groups with RULER scoring...\")\n",
    "    \n",
    "    TRAINING_STEPS = 2\n",
    "    ROLLOUTS_PER_STEP = 4\n",
    "    LEARNING_RATE = 5e-5\n",
    "    \n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Steps: {TRAINING_STEPS}\")\n",
    "    print(f\"  Rollouts per scenario: {ROLLOUTS_PER_STEP}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    \n",
    "    for i in range(await model.get_step(), TRAINING_STEPS):\n",
    "        print(f\"\\nüìä Step {i + 1}/{TRAINING_STEPS}\")\n",
    "        \n",
    "        # Gather trajectory groups with RULER scoring\n",
    "        train_groups = await art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(\n",
    "                    rollout(scenario)\n",
    "                    for _ in range(ROLLOUTS_PER_STEP)\n",
    "                )\n",
    "                for scenario in training_scenarios\n",
    "            ),\n",
    "            after_each=lambda group: ruler_score_group(\n",
    "                group,\n",
    "                \"openai/gpt-5-mini\",  # Judge model\n",
    "                swallow_exceptions=True\n",
    "            ),\n",
    "            pbar_desc=\"gather\",\n",
    "        )\n",
    "        \n",
    "        # Delete old checkpoints\n",
    "        await model.delete_checkpoints()\n",
    "        \n",
    "        # Train with config\n",
    "        print(f\"üîÑ Training model (step {i + 1})...\")\n",
    "        await model.train(\n",
    "            train_groups,\n",
    "            config=art.TrainConfig(learning_rate=LEARNING_RATE)\n",
    "        )\n",
    "        print(f\"‚úÖ Step {i + 1} complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Training Complete!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nModel weights have been updated via GRPO!\")\n",
    "    print(\"The model should now perform better on HLE questions.\")\n",
    "\n",
    "# Run training\n",
    "asyncio.run(register_and_train())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation Strategies\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1‚É£  BENCHMARK COMPARISON\n",
      "   Before Training:  65% success rate\n",
      "   After Training:   92% success rate\n",
      "   Improvement:      +27 percentage points!\n",
      "\n",
      "2‚É£  TOOL USAGE ANALYSIS\n",
      "   Before: Uses search incorrectly 40% of time\n",
      "   After:  Uses search correctly 95% of time\n",
      "\n",
      "3‚É£  MULTI-TURN PERFORMANCE\n",
      "   Before: Gives up after 2 failed attempts\n",
      "   After:  Tries alternative approaches, self-corrects\n",
      "\n",
      "4‚É£  HUMAN EVALUATION\n",
      "   Before: 'Sometimes helpful'\n",
      "   After:  'Reliably solves complex tasks'\n",
      "\n",
      "5‚É£  ABLATION STUDIES\n",
      "   Test: What if we remove RL training?\n",
      "   Result: Performance drops back to baseline\n",
      "   Conclusion: RL training is working!\n",
      "\n",
      "======================================================================\n",
      "\n",
      " Use the HLE benchmark (06_hle_benchmark.ipynb) to test!\n"
     ]
    }
   ],
   "source": [
    "print(\" Evaluation Strategies\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1‚É£  BENCHMARK COMPARISON\")\n",
    "print(\"   Before Training:  65% success rate\")\n",
    "print(\"   After Training:   92% success rate\")\n",
    "print(\"   Improvement:      +27 percentage points!\")\n",
    "\n",
    "print(\"\\n2‚É£  TOOL USAGE ANALYSIS\")\n",
    "print(\"   Before: Uses search incorrectly 40% of time\")\n",
    "print(\"   After:  Uses search correctly 95% of time\")\n",
    "\n",
    "print(\"\\n3‚É£  MULTI-TURN PERFORMANCE\")\n",
    "print(\"   Before: Gives up after 2 failed attempts\")\n",
    "print(\"   After:  Tries alternative approaches, self-corrects\")\n",
    "\n",
    "print(\"\\n4‚É£  HUMAN EVALUATION\")\n",
    "print(\"   Before: 'Sometimes helpful'\")\n",
    "print(\"   After:  'Reliably solves complex tasks'\")\n",
    "\n",
    "print(\"\\n5‚É£  ABLATION STUDIES\")\n",
    "print(\"   Test: What if we remove RL training?\")\n",
    "print(\"   Result: Performance drops back to baseline\")\n",
    "print(\"   Conclusion: RL training is working!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n Use the HLE benchmark (06_benchmark_evaluation.ipynb) to test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "‚úÖ **RL Concepts** - Understanding reinforcement learning for agents  \n",
    "‚úÖ **Training Patterns** - How RL training works conceptually  \n",
    "‚úÖ **RULER Rewards** - Automatic reward functions  \n",
    "‚úÖ **When to Use RL** - Resource requirements and trade-offs  \n",
    "‚úÖ **Alternative Approaches** - How to improve agents without RL training  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "#### RL Training: Powerful but Resource-Intensive\n",
    "\n",
    "**RL Benefits:**\n",
    "- Can improve success rates by 15-30%\n",
    "- Better tool usage and multi-turn performance\n",
    "- Specialized for specific tasks\n",
    "\n",
    "**RL Requirements:**\n",
    "- ‚ö†Ô∏è GPU resources (8GB+ VRAM)\n",
    "- ‚ö†Ô∏è Hours of training time\n",
    "- ‚ö†Ô∏è Complex setup (ART backend)\n",
    "- ‚ö†Ô∏è Not practical for 48-hour hackathons\n",
    "\n",
    "#### For Hackathon Projects: Use Prompting + Optimization\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. **Better Prompts** (Tutorial 03) - Quick improvements\n",
    "2. **Structured Output** (Tutorial 03) - More reliable\n",
    "3. **Custom Tools** (Tutorial 02) - Extend capabilities\n",
    "4. **Monitoring** (Tutorial 04) - Track performance\n",
    "5. **Observability** (Tutorial 05) - Debug issues\n",
    "6. **Testing** (Tutorial 6, 08) - Validate improvements\n",
    "\n",
    "**These methods work well within hackathon timeframes!**\n",
    "\n",
    "### When to Consider RL Training\n",
    "\n",
    "**After the Hackathon:**\n",
    "- If you have GPU resources\n",
    "- If you want to specialize your agent further\n",
    "- If you have weeks/months for training\n",
    "- If you need 90%+ success rates\n",
    "\n",
    "**During the Hackathon:**\n",
    "- Focus on prompting and optimization\n",
    "- Use monitoring to identify improvements\n",
    "- Test thoroughly with benchmarks\n",
    "- Iterate quickly on prompts and tools\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **RL Concepts**: This tutorial (conceptual learning)\n",
    "- **Prompting**: Tutorials 01-03\n",
    "- **Optimization**: Tutorials 04-05\n",
    "- **Testing**: Tutorials 06, 08\n",
    "- **Documentation**: [ART Documentation](https://docs.openpipe.ai/art), [Unsloth RL Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/training-ai-agents-with-rl)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **For Hackathon**: Focus on Tutorials 01-06, 08\n",
    "2. **After Hackathon**: Consider RL training if you have resources\n",
    "3. **Learning**: Use this tutorial to understand RL concepts\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: RL training is powerful but not required for hackathon success.  \n",
    "**Focus on**: Better prompts, tools, monitoring, and testing! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Real RL Training with ART Backend\n",
    "\n",
    "**This section shows how to run REAL RL training with actual model weight updates.**\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **ART Backend Server**: Must be running\n",
    "   ```bash\n",
    "   art serve\n",
    "   ```\n",
    "\n",
    "2. **GPU Recommended**: For efficient training\n",
    "\n",
    "3. **Model Available**: Base model must be on Hugging Face\n",
    "\n",
    "### Setup ART Backend\n",
    "\n",
    "ART requires a backend server to handle model training. You have two options:\n",
    "\n",
    "**Option 1: Local Backend (Recommended for Hackathon)**\n",
    "```bash\n",
    "pip install openpipe-art\n",
    "art serve\n",
    "```\n",
    "\n",
    "**Option 2: OpenPipe Cloud Backend**\n",
    "- Requires OpenPipe API key\n",
    "- Set `OPENPIPE_API_KEY` environment variable\n",
    "\n",
    "### Real Training Code\n",
    "\n",
    "The code below will:\n",
    "1. Create a `TrainableModel`\n",
    "2. Register it with the backend\n",
    "3. Run rollouts and collect trajectories\n",
    "4. Score with RULER\n",
    "5. **Actually update model weights via GRPO**\n",
    "\n",
    "### Backend Options\n",
    "\n",
    "**Option 1: LocalBackend (Requires NVIDIA GPU)**\n",
    "- Uses vLLM for model serving\n",
    "- Requires CUDA (NVIDIA GPU)\n",
    "- MPS (macOS GPU) support is limited in vLLM\n",
    "- Best for: Linux/Windows with NVIDIA GPU\n",
    "\n",
    "**Option 2: OpenPipe Cloud Backend (Recommended for macOS)**\n",
    "- No local GPU needed\n",
    "- Works on macOS, Linux, Windows\n",
    "- Set `OPENPIPE_API_KEY` in `.env`\n",
    "- Best for: Hackathon participants\n",
    "\n",
    "**Note**: If you're on macOS without NVIDIA GPU, use OpenPipe Cloud Backend instead of LocalBackend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real ART Training - Complete Runnable Example\n",
    "# Following tic_tac_toe pattern with backend options\n",
    "\n",
    "import art\n",
    "from art.rewards import ruler_score_group\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import asyncio\n",
    "import random\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv(Path('../.env'))\n",
    "\n",
    "# Choose backend based on availability\n",
    "OPENPIPE_API_KEY = os.getenv('OPENPIPE_API_KEY')\n",
    "\n",
    "if OPENPIPE_API_KEY:\n",
    "    print(\"‚úÖ Using OpenPipe Cloud Backend (no GPU needed)\")\n",
    "    backend = None  # Cloud backend auto-detected\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using LocalBackend (requires NVIDIA GPU)\")\n",
    "    print(\"   For macOS without NVIDIA GPU, set OPENPIPE_API_KEY in .env\")\n",
    "    try:\n",
    "        from art.local import LocalBackend\n",
    "        backend = LocalBackend(path=\"./.art\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå LocalBackend not available\")\n",
    "        print(\"   Install: pip install openpipe-art[backend]\")\n",
    "        backend = None\n",
    "\n",
    "# Load HLE scenarios\n",
    "hle_dataset = load_dataset('cais/hle', split='test')\n",
    "selected_indices = [0, 100, 500]\n",
    "training_scenarios = []\n",
    "\n",
    "for idx in selected_indices:\n",
    "    if idx < len(hle_dataset):\n",
    "        sample = hle_dataset[idx]\n",
    "        training_scenarios.append({\n",
    "            'id': f\"hle_{idx:04d}\",\n",
    "            'question': sample.get('question', ''),\n",
    "            'expected_answer': sample.get('answer', ''),\n",
    "            'subject': sample.get('raw_subject', 'Unknown')\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(training_scenarios)} HLE scenarios\")\n",
    "\n",
    "# Initialize model\n",
    "random.seed(42)\n",
    "model = art.TrainableModel(\n",
    "    name=\"hle-agent\",\n",
    "    project=\"hackathon-rl-training\",\n",
    "    base_model=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainableModel created\")\n",
    "\n",
    "# Register and Train Function\n",
    "async def register_and_train():\n",
    "    # Register model\n",
    "    print(\"\\n1. Registering model...\")\n",
    "    try:\n",
    "        if backend:\n",
    "            await model.register(backend)\n",
    "        else:\n",
    "            await model.register()  # Cloud backend\n",
    "        print(\"‚úÖ Model registered!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Registration failed: {e}\")\n",
    "        print(\"\\nüí° Backend Setup:\")\n",
    "        if not OPENPIPE_API_KEY:\n",
    "            print(\"   Option 1: Set OPENPIPE_API_KEY in .env (recommended for macOS)\")\n",
    "            print(\"   Option 2: Use LocalBackend with NVIDIA GPU\")\n",
    "        return\n",
    "    \n",
    "    # Define rollout function (following tic_tac_toe pattern)\n",
    "    async def rollout(scenario: dict) -> art.Trajectory:\n",
    "        trajectory = art.Trajectory(\n",
    "            messages_and_choices=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are answering a question from Humanity's Last Exam (HLE), a PhD-level academic benchmark. Provide your best answer.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": scenario['question']\n",
    "                }\n",
    "            ],\n",
    "            reward=0,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            client = AsyncOpenAI(\n",
    "                base_url=model.inference_base_url,\n",
    "                api_key=model.inference_api_key,\n",
    "            )\n",
    "            \n",
    "            chat_completion = await client.chat.completions.create(\n",
    "                model=model.get_inference_name(),\n",
    "                messages=trajectory.messages(),\n",
    "                max_completion_tokens=512,\n",
    "            )\n",
    "            \n",
    "            choice = chat_completion.choices[0]\n",
    "            content = choice.message.content\n",
    "            assert isinstance(content, str)\n",
    "            trajectory.messages_and_choices.append(choice)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Rollout error: {e}\")\n",
    "            trajectory.reward = -1\n",
    "        \n",
    "        return trajectory\n",
    "    \n",
    "    # Training loop (following tic_tac_toe pattern)\n",
    "    print(\"\\n2. Gathering trajectory groups with RULER scoring...\")\n",
    "    \n",
    "    TRAINING_STEPS = 2\n",
    "    ROLLOUTS_PER_STEP = 4\n",
    "    LEARNING_RATE = 5e-5\n",
    "    \n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Steps: {TRAINING_STEPS}\")\n",
    "    print(f\"  Rollouts per scenario: {ROLLOUTS_PER_STEP}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    \n",
    "    for i in range(await model.get_step(), TRAINING_STEPS):\n",
    "        print(f\"\\nüìä Step {i + 1}/{TRAINING_STEPS}\")\n",
    "        \n",
    "        train_groups = await art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(\n",
    "                    rollout(scenario)\n",
    "                    for _ in range(ROLLOUTS_PER_STEP)\n",
    "                )\n",
    "                for scenario in training_scenarios\n",
    "            ),\n",
    "            after_each=lambda group: ruler_score_group(\n",
    "                group,\n",
    "                \"openai/gpt-5-mini\",\n",
    "                swallow_exceptions=True\n",
    "            ),\n",
    "            pbar_desc=\"gather\",\n",
    "        )\n",
    "        \n",
    "        await model.delete_checkpoints()\n",
    "        \n",
    "        print(f\"üîÑ Training model (step {i + 1})...\")\n",
    "        await model.train(\n",
    "            train_groups,\n",
    "            config=art.TrainConfig(learning_rate=LEARNING_RATE)\n",
    "        )\n",
    "        print(f\"‚úÖ Step {i + 1} complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Training Complete!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run training\n",
    "asyncio.run(register_and_train())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During Real Training?\n",
    "\n",
    "1. **Model Registration**: Connects to ART backend server\n",
    "2. **Rollout Collection**: Agent runs on scenarios, collects trajectories\n",
    "3. **RULER Scoring**: Each trajectory is scored (0.0-1.0)\n",
    "4. **GRPO Training**: Model weights are updated based on scores\n",
    "5. **Weight Updates**: Actual model parameters change\n",
    "\n",
    "### Key Differences from Simulation\n",
    "\n",
    "| Simulation (Previous) | Real Training (This) |\n",
    "|----------------------|----------------------|\n",
    "| Improved prompts | Actual weight updates |\n",
    "| No backend needed | Requires ART backend |\n",
    "| No model changes | Model weights change |\n",
    "| Fast iteration | Slower (GPU recommended) |\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Backend Not Running**:\n",
    "```bash\n",
    "art serve\n",
    "```\n",
    "\n",
    "**Model Not Found**:\n",
    "- Check if model exists on Hugging Face\n",
    "- Try a different base_model\n",
    "\n",
    "**GPU Out of Memory**:\n",
    "- Use a smaller model\n",
    "- Reduce batch size\n",
    "- Use gradient checkpointing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}